\documentclass[a1paper,portrait,fontscale=0.43]{baposter}

\columnsep=70pt % This is the amount of white space between the columns in the poster
\columnseprule=3pt % This is the thickness of the black line between the columns in the poster

\usepackage{wrapfig}
\usepackage{lmodern}

\usepackage[utf8]{inputenc} %unicode support
\usepackage[T1]{fontenc}


\selectcolormodel{cmyk}

\graphicspath{{figures/}} % Directory in which figures are stored


\newcommand{\compresslist}{%
\setlength{\itemsep}{0pt}%
\setlength{\parskip}{1pt}%
\setlength{\parsep}{0pt}%
}

\newenvironment{boenumerate}
  {\begin{enumerate}\renewcommand\labelenumi{\textbf\theenumi.}}
  {\end{enumerate}}



\begin{document}


\definecolor{darkgreen}{cmyk}{0,0.90,0.74,0.28}
\definecolor{lightgreen}{cmyk}{0,0.90,0.74,0.28}

\begin{poster}
{
grid=false,
headerborder=open, % Adds a border around the header of content boxes
colspacing=1em, % Column spacing
bgColorOne=white, % Background color for the gradient on the left side of the poster
bgColorTwo=white, % Background color for the gradient on the right side of the poster
borderColor=darkgreen, % Border color
headerColorOne=lightgreen, % Background color for the header in the content boxes (left side)
headerColorTwo=lightgreen, % Background color for the header in the content boxes (right side)
headerFontColor=white, % Text color for the header text in the content boxes
boxColorOne=white, % Background color of the content boxes
textborder=rounded, %rectangle, % Format of the border around content boxes, can be: none, bars, coils, triangles, rectangle, rounded, roundedsmall, roundedright or faded
eyecatcher=false, % Set to false for ignoring the left logo in the title and move the title left
headerheight=0.15\textheight, % Height of the header
headershape=rounded, % Specify the rounded corner in the content box headers, can be: rectangle, small-rounded, roundedright, roundedleft or rounded
headershade=plain,
headerfont=\Large\textsf, % Large, bold and sans serif font in the headers of content boxes
%textfont={\setlength{\parindent}{1.5em}}, % Uncomment for paragraph indentation
linewidth=2pt % Width of the border lines around content boxes
}
{}
%
%----------------------------------------------------------------------------------------
%	TITLE AND AUTHOR NAME
%----------------------------------------------------------------------------------------
%
{  
  {\includegraphics[width=\textwidth]{DSP-02}} % SASE logo
  \textsf %Sans Serif  
  { Dynamic Reuse of Memory in 2D Convolution Applied to Image Processing}
} % Poster title
% {\vspace{1em} Marta Stepniewska, Pawel Siedlecki\\ % Author names
% {\small \vspace{0.7em} Department of Bioinformatics, Institute of Biochemistry and Biophysics, PAS, Warsaw, Pawinskiego 5a}} % Author email addresses
{\sf\vspace{0.5em}\\
    Martin Casabella,
    Sergio Sulca,
    Ivan Vignolles,
    Ariel L. Pola
\vspace{0.1em}\\
\small{Department of Research and Development Fundacion Fulgor 
\vspace{0.2em}\\
martin.casabella@gmail.com, ser.0090@gmail.com,
    ivanmvig@gmail.com and arielpola@gmail.com}
}
%{\includegraphics[width=0.1\textwidth]{logo}} % University/lab logo

{\small	
  \headerbox{1. Introduction}{name=introduction,column=0,row=0, span=3}{

%The origin of digital image processing is directly related to the development and evolution of computers. Most
%image filters that focus, blur, enhance edges and detect edges, among
%others, use convolution as a mathematical operation. Image processing is a very
%extensive research area with a large number of applications in multiple fields
%such as medicine, engineering, navigation, aeronautics, among others.
%Convolutional neural networks (CNN)~\cite{Lecun-et-al-1998} have received
%increasing attention in recent years. This technique uses a fast and efficient
%convolution implementation.

The implementation of high-speed image processing systems in field programmable
gate array (FPGA) has been a very active field. This is mainly due to the
ability to take advantage of bit level parallelism, pixel level, neighborhood
level and task level to increase computing performance and speed. %In addition, FPGAs are
%reconfigurable, allowing the flexibility that is often desired in neural
%networks. This combination of parallel processing and flexibility at very high
%speed, is what makes FPGAs a platform of choice for the development of these
%areas~\cite{papercnn}.

There are many examples in the literature of two-dimensional (2D) convolution
implementations, where most of the work focuses on high performance, resource
reduction and FPGA area efficiency. As shown in~\cite{paper3}, block random access
memory (BRAM) is one of the most used elements with the highest energy
consumption. Therefore, the architectures seek to reduce the use of BRAMs by
dividing them into two groups that consider either total storage or partial storage of the
image. The first of them increases processing by applying parallelization as
well as reusing resources to reduce complexity~\cite{paper1,paper5}. On the
other hand, architectures with partial storage partition the image into as many
parts as convolders are used~\cite{paper2,paper4}. The drawback presented by
these schemes is the non-modularization of processing, making it difficult to
improve the work rate by increasing parallelism.

In this work, we explain the concept of dynamic reuse of BRAM in 2D convolution
and its complexity concerning implementation for parallel architectures. Moreover, we
propose a modular architecture, where it is easy to appreciate that when the
parallelism level increases, the complexity of BRAM increases linearly.

%This paper is organized as follows. Section~\ref{sec:preproc} introduces the
%image processing algorithm. The archicterure design is
%presented in section~\ref{sec:architecture}. In
%section~\ref{sec:implementation}, hardware implementation and experimental
%results are shown. Finally, conclusions are drawn in
%section~\ref{sec:conclusion}.
}}


\headerbox{2. Materials and Method}{name=model,column=0,below=introduction,span=1}{

Python was the programming language utilised for all data and model analysis, and application development purposes. Some third-party libraries used in this experiment were: PyQt5, numpy, pandas, pickle, scikit-learn, xgboost, lightgbm, rgf, elm, keras and tensorflow.\\

All data was cleaned accordingly prior evaluation. Furthermore, by using cross-validation techniques, the data was split up into training and testing sets. The training of all models by fitting the training sets (all data split into partitions by cross-validation) was then followed by fitting the testing features to obtain a prediction set. In turn, by utilising these predictions for evaluation, some scoring metrics resulted for comparison.
\begin{center}
    \includegraphics[width=\linewidth]{cv}
\end{center}

%\begin{center}
%    \includegraphics[width=\linewidth]{kfoldrep}
%\end{center}
}

\headerbox{3. Results}{name=screen,span=2,column=1,below=introduction}{ % To reduce this block to 1 column width, remove 'span=2'

\begin{wrapfigure}{l}{0.35\textwidth}
    \vspace{10pt}
    \begin{center}
        \includegraphics[width=\linewidth]{r1}
    \end{center}
    %\vspace{-145pt}
\end{wrapfigure}

At every fold during the cross-validation process, each model returns a set of predictions, which the application stores. At the end of all iterations (10 iterations, since the cross-validation method implemented, is tuned to partition the data in 10 folds), the mean for every performance metric and each executed algorithm was calculated.

\hspace{0pt}\includegraphics[width=\linewidth]{r2}

}

\headerbox{4. Discussion}{name=sea,span=2,column=1,below=screen}{ % To reduce this block to 1 column width, remove 'span=2'

%  Similarly to what Percy et al. \cite{percy_predicting_2016} achieved,
The sequential NN model produced very close results to the RF with a mean accuracy score of 79.643\% and 82.585\% respectively. The ELM approach achieved comparable results to the NN. Sensitivity wise, the boosting methods LightGBM and XGBoost achieved the highest score out of the whole bunch as a result of 85.167\%. RF achieved the highest overall accuracy and AUROC score of 82.582\% and 0.8258 respectively. Concerning the sensitivity score, the least performing model was the GNB achieving an abysmal result of 24.72\%. The SVM model performed quite mediocre, achieving an overall AUROC and accuracy of 0.76724 and 75.204\% respectively.\\

\raggedright

Theil-Sen ranked with the set of lower performing models, achieving an overall AUROC of 0.683, with an even more inferior sensitivity score of 60.82\%. Similarly, LDA produced close AUROC results with an improved sensitivity value, still below average. Both the LogReg and LR models achieved better performance than the latter. Since LR managed a specificity value of only 59.13\%, this implies that 40.87\% samples were classified incorrectly. The K-Means clustering algorithm achieved the worst results. This technique is highly sensitive to data scaling and the range of values, which could explain the poor performance. Moreover, this algorithm also assumes that all features present are continuous, and even if transforming these variables to dummy-variables, this could negatively affect its performance.
%\begin{center}
%  \includegraphics[width=0.35\linewidth]{deep}
%\end{center}
}


{\small	
\headerbox{5. Conclusions}{name=conclusion,column=1,below=sea,span=2,above=bottom}{
% DeCAF is a chemoinformatical tool that can be helpful in ligand-based drug design.
% It provides a comprehensive molecule description and a fast algorithms for comparing and aligning multiple ligands.
%This research proves that Random Forests and the boosting methods LightGBM and XGBoost are amongst of the most highly effective techniques to predict and classify players as either exhibiting problematic behaviour characteristics or not.

%\begin{boenumerate}\compresslist
%    \item All three algorithms achieved over 80\% accuracy across all of the performance metrics calculated.
%    \item These three approaches achieved the top three execution times.
%    \item These three methods were the most consistent out of all the evaluated techniques.
%\end{boenumerate}

% It can be also used in other [procedures], such as database screening or drug repositioning.
% DeCAF is written in Python and freely available at \textbf{\color{darkgreen}http://bitbucket.org/marta-sd/decaf}. 
Using FPGA we are able to process the filtering at the same time as reading the
current image batch. In this paper, we have presented the implementation of
two-dimensional convolution on a Xilinx Artix 7 FPGA platform based on resource
efficiency and system parallelism. We implemented a whole image processing
system taking into account load stage, processing stage, and output stage.
Moreover, a relation between instantiated BRAM blocks and MAC units was
found in the presented architecture, which allows our system to work with different
parallelism degrees.

In addition, we optimized the use of memory resources implementing an algorithm
for memory operations and module synchronization. Performances and results show that resources utilization
concerning BRAM resources increase linearly, as desired.

High throughput was achieved in what processing is concerned. On the other hand,
the limiting factor that impacted the implemented system the most was UART speed
transmission. Another limiting factor was the DSP48A1 slices. Both factors
are easily solved by using an FPGA with more slices and
resources.
}}


\headerbox{6. References}{name=references,column=0,span=1,below=model,above=bottom}{


%\small % Reduce the font size in this block
\renewcommand{\section}[2]{\vskip 0.05em} % Get rid of the default "References" section title
%\nocite{*} % Insert publications even if they are not cited in the poster


\bibliographystyle{unsrt}
{\footnotesize
\bibliography{poster} % Use sample.bib as the bibliography file
}
}

\end{poster}

\end{document}
