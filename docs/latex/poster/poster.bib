@book{Goodfellow-et-al-2016,
  author =       {Goodfellow, Ian  and Bengio ,Yoshua and Courville, Aaron},
  title =        {Deep Learning},
  year =         2016,
  publisher =    {MIT Press},
  note =         {\url{http://www.deeplearningbook.org}},
  isbn={0-26-203561-8},
  edition   = 1,  
  pages =        329,
}

 @book{dinamic_rango,
    title = {Digital Image Processing},
    author = {Gonz\'alez, Rafael C.  and  Woods, Richard E.},
    publisher = {Pearson Prentice Hall},
    pages = {85},
    edition   = {3},	
	isbn = {0-13-168728-X},    
    year = {2008}
}

 @book{max_norm,
    title={Principles of Mathematical Analysis},
    author={Rudin, Walter},
    publisher={McGraw-Hill},
    pages={147-154},
    edition   = 3,
	isbn={0-07-054235-X},
    year={1976}
}

@article{fix_p,
  author  = {Cullen, Keith B. and Silvestre, Guenole C.M. and Hurley, Neil J. }, 
  title   = {Simulation Tools for Fixed Point DSP Algorithms and Architectures},
  journal = {International Journal of Signal Processing},
  year    = 2008,
  pages   = {199-203}, 
  volume  = 1,
  number  = 4
}

@mastersthesis{srntesis,
  title       = {An\'alisis del error en algoritmos de transmisi\'on de im\'agenes comprimidas con p\'erdida}, 
  author       = {Dionisio, Ram\'on, Hugo },
  school    = {Facultad de Inform\'atica, U.N.L.P},
  year         = 2002,
  address = {hramon@lidi.info.unlp.edu.ar}
}

@ARTICLE{Lecun-et-al-1998,
author={Lecun, Y.  and Bottou, L.  and Bengio, Y.  and Haffner, P. },
journal={Proceedings of the IEEE},
title={Gradient-based learning applied to document recognition},
year={1998},
volume={86},
number={11},
pages={2278-2324},
keywords={backpropagation;convolution;multilayer perceptrons;optical character recognition;2D shape variability;GTN;back-propagation;cheque reading;complex decision surface synthesis;convolutional neural network character recognizers;document recognition;document recognition systems;field extraction;gradient based learning technique;gradient-based learning;graph transformer networks;handwritten character recognition;handwritten digit recognition task;high-dimensional patterns;language modeling;multilayer neural networks;multimodule systems;performance measure minimization;segmentation recognition;Character recognition;Feature extraction;Hidden Markov models;Machine learning;Multi-layer neural network;Neural networks;Optical character recognition software;Optical computing;Pattern recognition;Principal component analysis},
doi={10.1109/5.726791},
ISSN={0018-9219},
month={Nov},
}


@techreport{paper1,
     title = {Implementation of 2D Convolution on FPGA, GPU and CPU},
     author = {Cope, Ben},
     group = {Department of Electrical & Electronic Engineering},
     year = {2010},
     institution = {UImperial College London},
     address = {benjamin.cope@imperial.ac.uk}
}

@mastersthesis{paper2,
  title       = {A Parallel FPGA
Implementation of Image
Convolution}, 
  author       = {Henrik Str\"om},
  school    = {Department of Electrical Engineering, Link\"oping University},
  year         = 2016,
}


@techreport{paper3,
     title = {Energy Efficient Image Convolution on FPGA},
     author = {Gupta, Agrim  and Prasanna, Viktor K.},
     group = {Department of Electrical Engineering},
     year = {2011},
     institution = {University of Southern California
Los Angeles, USA},
     address = {agrimgupta92@gmail.com, prasanna@usc.edu}
}
@mastersthesis{paper4,
  title       = {Parallel processing on FPGA for image convolution using Matlab}, 
  author       = {Ram\'irez, Diego and Romero, Ricardo  and Santa, Ferenando},
  school    = {Universidad Distrital Francisco Jos\'e de Caldas, Bogot\'a, Colombia},
  year         = 2015,
}

@article{paper5,
  author  = {kabbai, Leila and Sghaier, Anissa and Douik, Ali and Machhout, Mohsen }, 
  title   = {FPGA implementation of filtered image using 2D Gaussian filter},
  journal = {International Journal of Advanced Computer Science and Applications},
  year    = 2016,
  pages   = {514-520}, 
  volume  = 7,
  number  = 7
}
@inproceedings{papercnn,
 author = {Zhang, Chen and Li, Peng and Sun, Guangyu and Guan, Yijin and Xiao, Bingjun and Cong, Jason},
 title = {Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks},
 booktitle = {Proceedings of the 2015 ACM/SIGDA International Symposium on Field-Programmable Gate Arrays},
 series = {FPGA '15},
 year = {2015},
 isbn = {978-1-4503-3315-3},
 location = {Monterey, California, USA},
 pages = {161--170},
 numpages = {10},
 url = {http://doi.acm.org/10.1145/2684746.2689060},
 doi = {10.1145/2684746.2689060},
 acmid = {2689060},
 publisher = {ACM},
 address = {New York, NY, USA},
 keywords = {acceleration, convolutional neural network, fpga, roofline model},
}

@inproceedings{validconv,
  author    = {Saxe, Andrew M.  and
               Koh, Pang Wei  and
               Chen, Zhenghao  and
               Bhand, Maneesh and
               Suresh, Bipin  and
               Ng, Andrew Y.},
  title     = {On Random Weights and Unsupervised Feature Learning},
  booktitle = {{ICML}},
  pages     = {1089--1096},
  publisher = {Omnipress},
  year      = {2011}
}







%del poster original.............................
@article{wells_how_2016,
	title = {How to beat an online gambling addiction},
	issn = {0307-1235},
	url = {http://www.telegraph.co.uk/men/the-filter/how-to-beat-an-online-gambling-addiction/},
	abstract = {Earlier this week, it was revealed that a 23-year old accountant who plunged to his death from a London skyscraper last summer \&quot;died of shame\&quot; from his online gambling addiction.},
	language = {en-GB},
	urldate = {2017-10-18},
	journal = {The Telegraph},
	author = {Wells, Jonathan},
	month = apr,
	year = {2016},
	keywords = {Addiction, The filter, Men, Thinking man, Standard},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/SFUB6AKW/how-to-beat-an-online-gambling-addiction.html:text/html}
}

@article{moore_prediction_1999,
	title = {The prediction of gambling behaviour and problem gambling from attitudes and perceived norms.},
	volume = {27},
	doi = {10.2224/sbp.1999.27.5.455},
	abstract = {The aims of this study were to characterise gambling attitudes and social norms among adult Australians, and to evaluate whether gambling behavior (frequency) and problem gambling could be predicted by a model combining attitudes and social influences. With a sample of 215 late adolescents
and adults, the Theory of Reasoned Action was found significantly to predict gambling frequency and problem gambling, with intentions to gamble predicting behavior, subjective norms predicting intentions (and gambling frequency), and attitudes predicting intentions. Males scored higher than
females on both problem gambling and gambling frequency. Across the sample, although most had gambled at some time (89 per cent), gambling frequency and problem gambling were low, and attitudes and subjective norms with respect to gambling were a complex mixture of acceptance and rejection.},
	number = {5},
	journal = {Social Behavior and Personality: an international journal},
	author = {Moore, Susan M. and Ohtsuka, Keis},
	month = jan,
	year = {1999},
	pages = {455--466}
}

@article{philander_identifying_2014,
	title = {Identifying high-risk online gamblers: a comparison of data mining procedures},
	volume = {14},
	issn = {1445-9795},
	shorttitle = {Identifying high-risk online gamblers},
	url = {http://dx.doi.org/10.1080/14459795.2013.841721},
	doi = {10.1080/14459795.2013.841721},
	abstract = {Using play data from a sample of virtual live action sports betting gamblers, this study evaluates a set of classification and regression algorithms to determine which techniques are more effective in identifying probable disordered gamblers. This study identifies a clear need for validating results using players not appearing in the original sample, as even methods that use in-sample cross-validation can show substantial differences in performance from one data set to another. Many methods are found to be quite accurate in correctly identifying player types in training data, but perform poorly when used on new samples. Artificial neural networks appear to be the most reliable classification method overall, but still fail to identify a large group of likely problem gamblers. Bet intensity, variability, frequency and trajectory, as well as age and gender are noted to be insufficient variables to classify probable disordered gamblers with arbitrarily reasonable accuracy.},
	number = {1},
	urldate = {2017-10-18},
	journal = {International Gambling Studies},
	author = {Philander, Kahlil S.},
	month = jan,
	year = {2014},
	keywords = {supervised learning algorithm, responsible gambling, classification, internet gambling},
	pages = {53--63},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/5VXZCDJI/14459795.2013.html:text/html}
}

@article{ng_addiction_2005,
	title = {Addiction to the {Internet} and {Online} {Gaming}},
	volume = {8},
	issn = {1094-9313},
	url = {http://online.liebertpub.com/doi/abs/10.1089/cpb.2005.8.110},
	doi = {10.1089/cpb.2005.8.110},
	abstract = {As computer and Internet use become a staple of everyday life, the potential for overuse is introduced, which may lead to addiction. Research on Internet addiction has shown that users can become addicted to it. Addiction to the Internet shares some of the negative aspects of substance addiction and has been shown to lead to consequences such as failing school, family, and relationship problems.},
	number = {2},
	urldate = {2017-10-18},
	journal = {CyberPsychology \& Behavior},
	author = {Ng, Brian D. and Wiemer-Hastings, Peter},
	month = apr,
	year = {2005},
	pages = {110--113},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/86JCN2WU/cpb.2005.8.html:text/html}
}

@article{griffiths_social_2002,
	title = {The {Social} {Impact} of {Internet} {Gambling}},
	volume = {20},
	issn = {0894-4393},
	url = {http://journals.sagepub.com/doi/abs/10.1177/089443930202000308},
	doi = {10.1177/089443930202000308},
	abstract = {Technology has always played a role in the development of gambling practices and continues to provide new market opportunities. One of the fastest growing areas is that of Internet gambling. The effect of such technologies should not be accepted uncritically, particularly as there may be areas of potential concern based on what is known about problem gambling offline. This article has three aims. First, it overviews some of the main social concerns about the rise of Internet gambling. Second, it looks at the limited research that has been carried out in this area. Third, it examines whether Internet gambling is doubly addictive, given research that suggests that the Internet can be addictive itself. It is concluded that technological developments in Internet gambling will increase the potential for problem gambling globally, but that many of the ideas and speculations outlined in this article need to be addressed further by large-scale empirical studies.},
	language = {en},
	number = {3},
	urldate = {2017-10-18},
	journal = {Social Science Computer Review},
	author = {Griffiths, Mark D. and Parke, Jonathan},
	month = aug,
	year = {2002},
	pages = {312--320}
}

@article{gainsbury_online_2015,
	title = {Online {Gambling} {Addiction}: the {Relationship} {Between} {Internet} {Gambling} and {Disordered} {Gambling}},
	volume = {2},
	issn = {2196-2952},
	shorttitle = {Online {Gambling} {Addiction}},
	url = {https://link.springer.com/article/10.1007/s40429-015-0057-8},
	doi = {10.1007/s40429-015-0057-8},
	abstract = {One of the most significant changes to the gambling environment in the past 15 years has been the increased availability of Internet gambling, including mobile; Internet gambling is the fastest growing mode of gambling and is changing the way that gamblers engage with this activity. Due to the high level of accessibility, immersive interface and ease at which money can be spent, concerns have been expressed that Internet gambling may increase rates of disordered gambling. The current paper aimed to provide an overview of the research to date as well as highlight new and interesting findings relevant to Internet gambling addiction. A comprehensive review of the existing literature was conducted to provide an overview of significant trends and developments in research that relates to disordered Internet gambling. This paper presents research to inform a greater understanding of adult participation in Internet gambling, features of this interface that may impact problem severity, the relationship between Internet gambling and related problems, as well as considering the role of the wider spectrum of gambling behaviour and relevant individual factors that moderate this relationship.},
	language = {en},
	number = {2},
	urldate = {2017-10-18},
	journal = {Curr Addict Rep},
	author = {Gainsbury, Sally M.},
	month = jun,
	year = {2015},
	pages = {185--193},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/59TFUMWK/Gainsbury - 2015 - Online Gambling Addiction the Relationship Betwee.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/THNKUSKH/s40429-015-0057-8.html:text/html}
}

@article{griffiths_social_2009,
	title = {Social {Responsibility} {Tools} in {Online} {Gambling}: {A} {Survey} of {Attitudes} and {Behavior} among {Internet} {Gamblers}},
	volume = {12},
	issn = {1094-9313},
	shorttitle = {Social {Responsibility} {Tools} in {Online} {Gambling}},
	url = {http://online.liebertpub.com/doi/abs/10.1089/cpb.2009.0062},
	doi = {10.1089/cpb.2009.0062},
	abstract = {To date, little empirical research has focused on social responsibility in gambling. This study examined players' attitudes and behavior toward using the social responsibility tool PlayScan designed by the Swedish gaming company Svenska Spel. Via PlayScan, players have the option to utilize various social responsibility control tools (e.g., personal gaming budgets, self-diagnostic tests of gambling habits, self-exclusion options). A total of 2,348 participants took part in an online questionnaire study. Participants were clientele of the Svenska Spel online gambling Web site. Results showed that just over a quarter of players (26\%) had used PlayScan. The vast majority of those who had activated PlayScan (almost 9 in 10 users) said that PlayScan was easy to use. Over half of PlayScan users (52\%) said it was useful; 19\% said it was not. Many features were seen as useful by online gamblers, including limit setting (70\%), viewing their gambling profile (49\%), self-exclusion facilities (42\%), self-diagnostic problem gambling tests (46\%), information and support for gambling issues (40\%), and gambling profile predictions (36\%). In terms of actual (as opposed to theoretical) use, over half of PlayScan users (56\%) had set spending limits, 40\% had taken a self-diagnostic problem gambling test, and 17\% had used a self-exclusion feature.},
	number = {4},
	urldate = {2017-10-25},
	journal = {CyberPsychology \& Behavior},
	author = {Griffiths, Mark D. and Wood, Richard T.A. and Parke, Jonathan},
	month = jul,
	year = {2009},
	pages = {413--421},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/T4RDBXI6/cpb.2009.html:text/html}
}

@article{griffiths_dsm-5_2014,
	title = {{DSM}-5 {Internet} {Gaming} {Disorder} needs a unified approach to assessment},
	volume = {4},
	doi = {10.2217/npy.13.82},
	abstract = {be included as a separate mental disorder until the defining features of IGD have been identified, reliability and validity of specific IGD criteria have been obtained cross-culturally, prevalence rates have been determined in representative epidemiologi -cal samples across the world, and etiology and associated biological features have been evaluated},
	journal = {Neuropsychiatry},
	author = {Griffiths, Mark and King, Daniel and Demetrovics, Zsolt},
	month = feb,
	year = {2014},
	pages = {1--4},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/UFEX6BUR/Griffiths et al. - 2014 - DSM-5 Internet Gaming Disorder needs a unified app.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/KKU9USHZ/259399635_DSM-5_Internet_Gaming_Disorder_needs_a_unified_approach_to_assessment.html:text/html}
}

@misc{carter_responsible_2016,
	type = {Blog},
	title = {Responsible {Gaming} {Conference}},
	url = {http://www.rgcmalta.com/speaker/liz-carter/},
	urldate = {2017-10-30},
	author = {Carter, Liz},
	year = {2016},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/7MDS79X6/liz-carter.html:text/html}
}

@inproceedings{carter_understanding_2016,
	address = {Malta},
	title = {An {Understanding} of {Gambling} {Addiction} in {Women}},
	url = {http://www.rgcmalta.com/speaker/liz-carter/},
	author = {Carter, Liz},
	year = {2016}
}

@inproceedings{derevensky_everyones_2008,
	address = {Slovenia},
	title = {Everyone’s a winner! {Youth} perceptions of gambling advertisements},
	url = {http://www.easg.org/website/conference.cfm?id=14&cid=14&section=PRESENTATIONS},
	publisher = {European Association for the Study of Gambling},
	author = {Derevensky, Jeffrey},
	month = jul,
	year = {2008}
}

@book{goodman_addiction:_1990,
	title = {Addiction: definition and implications},
	volume = {85},
	shorttitle = {Addiction},
	abstract = {Integration of addiction into the theory and practice of psychiatry has been hampered by the lack of a definition of addiction which is scientifically useful. A definition is proposed, with diagnostic criteria specified in a format similar to that of DSM-III-R. Essentially, addiction designates a process whereby a behavior, that can function both to produce pleasure and to provide escape from internal discomfort, is employed in a pattern characterized by (1) recurrent failure to control the behaviour (powerlessness) and (2) continuation of the behaviour despite significant negative consequences (unmanageability). Some practical and theoretical implications of this definition are then explored.},
	author = {Goodman, Aviel},
	month = dec,
	year = {1990}
}

@misc{noauthor_gambling_nodate,
	title = {Gambling addiction: {Symptoms}, statistics, and treatment},
	shorttitle = {Gambling addiction},
	url = {https://www.medicalnewstoday.com/articles/15929.php},
	abstract = {We take a close look at gambling addiction - typically a progressive addiction that can have many negative psychological, physical and social repercussions.},
	urldate = {2017-11-06},
	journal = {Medical News Today},
	author = {Nordqvist, Christian},
	month = jun,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/C6JUZ9AZ/15929.html:text/html}
}

@misc{noauthor_5_2014,
	title = {5 {Alarming} {Gambling} {Addiction} {Statistics}},
	url = {https://www.addictions.com/gambling/5-alarming-gambling-addiction-statistics/},
	abstract = {These gambling addiction statistics show the alarming extent of gambling in the United States.},
	urldate = {2017-11-06},
	journal = {Addictions},
	author = {{addictions.com}},
	year = {2014},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/FG6CCAVQ/5-alarming-gambling-addiction-statistics.html:text/html}
}

@misc{noauthor_gambling_nodate-1,
	title = {Gambling {Addiction} {Statistics}},
	url = {https://rehab-international.org/gambling-addiction/gambling-addiction-statistics},
	abstract = {Here are some informative and interesting stats that surround gambling addiction. If you need help with an addiction, call to speak to a rehab expert today.},
	urldate = {2017-11-06},
	journal = {Rehab International},
	author = {{Rehab International}},
	year = {2010},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/9QAFVP4H/gambling-addiction-statistics.html:text/html}
}

@techreport{noauthor_global_2011,
	address = {Castletown, Isle of Man, British Isles},
	title = {Global {Gaming} {Report} 6th {Edition}},
	url = {http://www.gbgc.com/index.html},
	number = {6th Edition},
	institution = {Global Betting and Gaming Consultants},
	author = {{GBGC}},
	year = {2011}
}

@article{labrie_assessing_2007,
	title = {Assessing the playing field: a prospective longitudinal study of internet sports gambling behavior},
	volume = {23},
	issn = {1573-3602},
	shorttitle = {Assessing the playing field},
	doi = {10.1007/s10899-007-9067-3},
	abstract = {Internet gambling is growing rapidly, as is concern about its possible effect on the public's health. This paper reports the results of the first prospective longitudinal study of actual Internet sports gambling behavior during eight study months. Data include recorded fixed-odds bets on the outcome of sporting contests and live-action bets on the outcome of events within contests for 40,499 Internet sports gambling service subscribers who enrolled during February 2005. We tracked the following primary gambling behaviors: daily totals of the number of bets made, money bet, and money won. We transformed these variables into measures of gambling involvement. We analyzed behavior for both fixed-odds and live-action bets. The median betting behavior of the 39,719 fixed-odds bettors was to place 2.5 bets of 4 euro (approximately \$5.3 US) every fourth day during the median 4 months from first to last bet. This typical pattern incurred a loss of 29\% of the amount wagered. The median betting behavior of the 24,794 live-action bettors was to place 2.8 wagers of 4 euro every fourth day during the median duration of 6 weeks at a loss of 18\% of the amount wagered. We also examined the behavior of empirically determined groups of heavily involved bettors whose activity exceeded that of 99\% of the sample.},
	language = {eng},
	number = {3},
	journal = {J Gambl Stud},
	author = {LaBrie, Richard A. and LaPlante, Debi A. and Nelson, Sarah E. and Schumann, Anja and Shaffer, Howard J.},
	month = sep,
	year = {2007},
	pmid = {17574522},
	keywords = {Behavior, Addictive, Gambling, Humans, Internal-External Control, Internet, Longitudinal Studies, Prospective Studies, Reward, Risk-Taking, Self Efficacy, Sports, Surveys and Questionnaires},
	pages = {347--362}
}

@misc{macgregor_machine_2015,
	title = {Machine learning to help predict online gambling addiction},
	url = {https://thestack.com/cloud/2015/10/26/machine-learning-to-help-predict-online-gambling-addiction/},
	abstract = {A new system designed to help users control their online gambling, provides an ‘early warning’ notification to gamblers who show signs of addiction.},
	urldate = {2017-11-14},
	journal = {The Stack},
	author = {MacGregor, Alice},
	month = oct,
	year = {2015},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/RUB2ZD7U/machine-learning-to-help-predict-online-gambling-addiction.html:text/html}
}

@article{martin_using_2010,
	title = {Using the theory of planned behavior to predict gambling behavior},
	volume = {24},
	issn = {1939-1501},
	doi = {10.1037/a0018452},
	abstract = {Gambling is an important public health concern. To better understand gambling behavior, we conducted a classroom-based survey that assessed the role of the theory of planned behavior (TPB; i.e., intentions, subjective norms, perceived behavioral control, and attitudes) in past-year gambling and gambling frequency among college students. Results from this research support the utility of the TPB to explain gambling behavior in this population. Specifically, in TPB models to predict gambling behavior, friend and family subjective norms and perceived behavioral control predicted past-year gambling, and friend and family subjective norms, attitudes, and perceived behavioral control predicted gambling frequency. Intention to gamble mediated these relationships. These findings suggest that college-based responsible gambling efforts should consider targeting misperceptions of approval regarding gambling behavior (i.e., subjective norms), personal approval of gambling behavior (i.e., attitudes), and perceived behavioral control to better manage gambling behavior in various situations. (PsycINFO Database Record (c) 2010 APA, all rights reserved).},
	language = {eng},
	number = {1},
	journal = {Psychol Addict Behav},
	author = {Martin, Ryan J. and Usdan, Stuart and Nelson, Sarah and Umstattd, M. Renee and Laplante, Debi and Perko, Mike and Shaffer, Howard},
	month = mar,
	year = {2010},
	pmid = {20307115},
	keywords = {Gambling, Humans, Prospective Studies, Surveys and Questionnaires, Adolescent, Adult, Disruptive, Impulse Control, and Conduct Disorders, Female, Male, Predictive Value of Tests, Psychological Theory, Students, Young Adult},
	pages = {89--97}
}

@article{kanungo_efficient_2002,
	title = {An efficient k-means clustering algorithm: analysis and implementation},
	volume = {24},
	issn = {0162-8828},
	shorttitle = {An efficient k-means clustering algorithm},
	doi = {10.1109/TPAMI.2002.1017616},
	abstract = {In k-means clustering, we are given a set of n data points in d-dimensional space Rd and an integer k and the problem is to determine a set of k points in Rd, called centers, so as to minimize the mean squared distance from each data point to its nearest center. A popular heuristic for k-means clustering is Lloyd's (1982) algorithm. We present a simple and efficient implementation of Lloyd's k-means clustering algorithm, which we call the filtering algorithm. This algorithm is easy to implement, requiring a kd-tree as the only major data structure. We establish the practical efficiency of the filtering algorithm in two ways. First, we present a data-sensitive analysis of the algorithm's running time, which shows that the algorithm runs faster as the separation between clusters increases. Second, we present a number of empirical studies both on synthetically generated data and on real data sets from applications in color quantization, data compression, and image segmentation},
	number = {7},
	journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
	author = {Kanungo, T. and Mount, D. M. and Netanyahu, N. S. and Piatko, C. D. and Silverman, R. and Wu, A. Y.},
	month = jul,
	year = {2002},
	keywords = {covariance matrices, filtering theory, pattern clustering, Lloyd algorithm, color quantization, data compression, data structure, data-sensitive analysis, filtering algorithm, image segmentation, k-means clustering algorithm, kd-tree, mean squared distance, Algorithm design and analysis, Clustering algorithms, Data analysis, Data mining, Data structures, Filtering algorithms, Iterative algorithms, Machine learning algorithms, Pattern recognition},
	pages = {881--892},
	file = {IEEE Xplore Abstract Record:/Users/davidfarrugia/Zotero/storage/Q6UGCCXF/1017616.html:text/html}
}

@article{freund_short_1999,
	title = {A {Short} {Introduction} to {Boosting}},
	volume = {14},
	abstract = {Boosting is a general method for improving the accuracy of any given learning algorithm. This short overview paper introduces the boosting algorithm AdaBoost, and explains the un-derlying theory of boosting, including an explanation of why boosting often does not suffer from overfitting as well as boosting's relationship to support-vector machines. Some examples of recent applications of boosting are also described.},
	journal = {Journal of Japanese Society for Artificial Intelligence},
	author = {Freund, Yoav and E Schapire, Robert},
	month = oct,
	year = {1999},
	pages = {771--780}
}

@article{alsabti_efficient_1997,
	title = {An efficient k-means clustering algorithm},
	url = {https://surface.syr.edu/eecs/43},
	journal = {Electrical Engineering and Computer Science},
	author = {Alsabti, Khaled and Ranka, Sanjay and Singh, Vineet},
	month = jan,
	year = {1997},
	file = {"An efficient k-means clustering algorithm" by Khaled Alsabti, Sanjay Ranka et al.:/Users/davidfarrugia/Zotero/storage/DFXDI9MH/43.html:text/html}
}

@article{coussement_customer_2013,
	series = {Advancing {Research} {Methods} in {Marketing}},
	title = {Customer churn prediction in the online gambling industry: {The} beneficial effect of ensemble learning},
	volume = {66},
	issn = {0148-2963},
	shorttitle = {Customer churn prediction in the online gambling industry},
	url = {http://www.sciencedirect.com/science/article/pii/S0148296312003530},
	doi = {10.1016/j.jbusres.2012.12.008},
	abstract = {The online gambling industry is one of the most revenue generating branches of the entertainment business, resulting in fierce competition and saturated markets. Therefore it is essential to efficiently retain gamblers. Churn prediction is a promising new alternative in customer relationship management (CRM) to analyze customer retention. It is the process of identifying gamblers with a high probability to leave the company based on their past behavior. This study investigates whether churn prediction is a valuable option in the CRM palette of the online gambling companies. Using real-life data of poker players at bwin, single algorithms, CART decision trees and generalized additive models are benchmarked to their ensemble counterparts, random forests and GAMens. The results show that churn prediction is a valuable strategy to identify and profile those customers at risk. Furthermore, the performance of the ensembles is more robust and better than the single models.},
	number = {9},
	urldate = {2017-11-15},
	journal = {Journal of Business Research},
	author = {Coussement, Kristof and De Bock, Koen W.},
	month = sep,
	year = {2013},
	keywords = {Customer relationship management, Online gambling, Customer churn prediction, Ensemble algorithms, GAMens, Random forests},
	pages = {1629--1636},
	file = {ScienceDirect Snapshot:/Users/davidfarrugia/Zotero/storage/U6K8RXJH/S0148296312003530.html:text/html}
}

@article{markham_detection_2013,
	title = {Detection of {Problem} {Gambler} {Subgroups} {Using} {Recursive} {Partitioning}},
	volume = {11},
	issn = {1557-1874, 1557-1882},
	url = {https://link.springer.com/article/10.1007/s11469-012-9408-z},
	doi = {10.1007/s11469-012-9408-z},
	abstract = {The multivariate socio-demographic risk factors for problem gambling have been well documented. While this body of research is valuable in determining risk factors aggregated across various populations, the majority of studies tend not to specifically identify particular subgroups of problem gamblers based on the interaction between variables. The identification of problem gambling subgroups offers the potential for improved harm-reduction initiatives in particular geographic contexts. We introduce an analytical approach termed recursive partitioning, commonly used in the health sciences but infrequently employed in gambling research, to identify specific gambler subgroups based on the interaction of a range of predictor variables. Recursive partitioning creates groups of cases (e.g. gamblers) with similar outcomes by repeatedly splitting each group into smaller and more homogenous subgroups. We employ it to define problem gambler subgroups within a diverse population context (i.e. northern Australia) and compare the results with a multivariate analysis of the same dataset using a generalized linear regression model. We assess the advantages and disadvantages of each approach, and argue that recursive partitioning is an easily-interpretable approach that may be useful both in identifying problem gambling subgroups and in developing targeted harm-minimisation strategies.},
	language = {en},
	number = {3},
	urldate = {2017-11-15},
	journal = {Int J Ment Health Addiction},
	author = {Markham, Francis and Young, Martin and Doran, Bruce},
	month = jun,
	year = {2013},
	pages = {281--291},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/SEKQ45SV/s11469-012-9408-z.html:text/html}
}

@misc{trevino_introduction_2016,
	title = {Introduction to {K}-means {Clustering}},
	url = {https://www.datascience.com/learn-data-science/tutorials/introduction-to-k-means-clustering-algorithm-data-science},
	abstract = {Learn data science with data scientist Dr. Andrea Trevino's step-by-step tutorial on the K-means clustering unsupervised machine learning algorithm.},
	urldate = {2017-11-16},
	journal = {DataScience},
	author = {Trevino, Andrea},
	year = {2016},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/XRABUH3Q/introduction-to-k-means-clustering-algorithm-data-science.html:text/html}
}

@article{schapire_boostexter:_2000,
	title = {{BoosTexter}: {A} {Boosting}-based {System} for {Text} {Categorization}},
	volume = {39},
	issn = {0885-6125, 1573-0565},
	shorttitle = {{BoosTexter}},
	url = {https://link.springer.com/article/10.1023/A:1007649029923},
	doi = {10.1023/A:1007649029923},
	abstract = {This work focuses on algorithms which learn from examples to perform multiclass text and speech categorization tasks. Our approach is based on a new and improved family of boosting algorithms. We describe in detail an implementation, called BoosTexter, of the new boosting algorithms for text categorization tasks. We present results comparing the performance of BoosTexter and a number of other text-categorization algorithms on a variety of tasks. We conclude by describing the application of our system to automatic call-type identification from unconstrained spoken customer responses.},
	language = {en},
	number = {2-3},
	urldate = {2017-11-16},
	journal = {Machine Learning},
	author = {Schapire, Robert E. and Singer, Yoram},
	month = may,
	year = {2000},
	pages = {135--168},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/PI58QHKS/Schapire and Singer - 2000 - BoosTexter A Boosting-based System for Text Categ.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/XKPJTAKI/A1007649029923.html:text/html}
}

@book{schank_dynamic_1983,
	address = {New York, NY, USA},
	title = {Dynamic {Memory}: {A} {Theory} of {Reminding} and {Learning} in {Computers} and {People}},
	isbn = {978-0-521-24858-7},
	shorttitle = {Dynamic {Memory}},
	publisher = {Cambridge University Press},
	author = {Schank, Roger C.},
	year = {1983}
}

@article{aamodt_case-based_1994,
	title = {Case-{Based} {Reasoning}: {Foundational} {Issues}, {Methodological} {Variations}, and {System} {Approaches}},
	volume = {7},
	shorttitle = {Case-{Based} {Reasoning}},
	number = {1},
	journal = {Artificial Intelligence Communications},
	author = {Aamodt, Agnar and Plaza, Enric},
	year = {1994},
	pages = {39--59},
	file = {Case-Based Reasoning\: Foundational Issues, Methodological Variations, and System Approaches | IIIA:/Users/davidfarrugia/Zotero/storage/6E4QKVG9/case-based-reasoning-foundational-issues-methodological-variations-and-system-approache.html:text/html}
}

@article{wittig_estimating_1997,
	title = {Estimating software development effort with connectionist models},
	volume = {39},
	issn = {0950-5849},
	url = {http://www.sciencedirect.com/science/article/pii/S0950584997000049},
	doi = {10.1016/S0950-5849(97)00004-9},
	abstract = {Accurate software development effort estimation is important for effective project management. Research studies indicate that effort estimation is a complex issue and results have in general not been encouraging. Artificial Neural Networks are recognised for their ability to provide good results when dealing with problems where there are complex relationships between inputs and outputs, and where the input data is distorted by high noise levels. This paper reports on the assessment of back-propagation neural network models for effort estimation. The models were tested on simulated data as well as actual data of commercial projects. This project data had large productivity variations, noise and missing data values, which enabled model evaluation under typical software development conditions. The results were encouraging, with the networks showing an ability to estimate development effort within 25\% of actual effort more than 75\% of the time for one large commercial data set.},
	number = {7},
	urldate = {2017-11-16},
	journal = {Information and Software Technology},
	author = {Wittig, Gerhard and Finnie, Gavin},
	month = jan,
	year = {1997},
	keywords = {Software estimation, Artificial neural networks, Metrics},
	pages = {469--476},
	file = {ScienceDirect Snapshot:/Users/davidfarrugia/Zotero/storage/J9MAZJH2/S0950584997000049.html:text/html}
}

@misc{noauthor_optimal_nodate,
	title = {Optimal {Tuning} {Parameters}},
	url = {http://www.ritchieng.com/machine-learning-efficiently-search-tuning-param/},
	abstract = {I am Ritchie Ng, a machine learning engineer specializing in deep learning and computer vision. Check out my code guides and keep ritching for the skies!},
	urldate = {2017-11-17},
	journal = {ritchieng.github.io},
	author = {Ng, Ritchie},
	month = jul,
	year = {2016},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/45UQUATQ/machine-learning-efficiently-search-tuning-param.html:text/html}
}

@incollection{snoek_practical_2012,
	title = {Practical {Bayesian} {Optimization} of {Machine} {Learning} {Algorithms}},
	url = {http://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf},
	urldate = {2017-11-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	pages = {2951--2959},
	file = {NIPS Snapshort:/Users/davidfarrugia/Zotero/storage/RUF5GT4R/4522-practical-bayesian-optimization-of-machine-l.html:text/html}
}

@incollection{mockus_application_2014,
	title = {The application of {Bayesian} methods for seeking the extremum},
	volume = {2},
	isbn = {978-0-444-85171-0},
	abstract = {The objective function of the multi-extremal optimisation problem is modelled by the realisation of some stochastic function. The sequence of solution points seeks to minimise the expected deviation of the objective record from the extremum.},
	booktitle = {Towards {Global} {Optimization}},
	author = {Mockus, J and Tiesis, Vytautas and Zilinskas, Antanas},
	month = sep,
	year = {2014},
	pages = {117--129},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/VXWDFZSD/Mockus et al. - 2014 - The application of Bayesian methods for seeking th.pdf:application/pdf}
}

@article{jones_taxonomy_2001,
	title = {A {Taxonomy} of {Global} {Optimization} {Methods} {Based} on {Response} {Surfaces}},
	volume = {21},
	issn = {0925-5001, 1573-2916},
	url = {https://link.springer.com/article/10.1023/A:1012771025575},
	doi = {10.1023/A:1012771025575},
	abstract = {This paper presents a taxonomy of existing approaches for using response surfaces for global optimization. Each method is illustrated with a simple numerical example that brings out its advantages and disadvantages. The central theme is that methods that seem quite reasonable often have non-obvious failure modes. Understanding these failure modes is essential for the development of practical algorithms that fulfill the intuitive promise of the response surface approach.},
	language = {en},
	number = {4},
	urldate = {2017-11-17},
	journal = {Journal of Global Optimization},
	author = {Jones, Donald R.},
	month = dec,
	year = {2001},
	pages = {345--383},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/CDEP2XMD/A1012771025575.html:text/html}
}

@article{krause_near-optimal_2008,
	title = {Near-{Optimal} {Sensor} {Placements} in {Gaussian} {Processes}: {Theory}, {Efficient} {Algorithms} and {Empirical} {Studies}},
	volume = {9},
	issn = {1532-4435},
	shorttitle = {Near-{Optimal} {Sensor} {Placements} in {Gaussian} {Processes}},
	url = {http://dl.acm.org/citation.cfm?id=1390681.1390689},
	abstract = {When monitoring spatial phenomena, which can often be modeled as Gaussian processes (GPs), choosing sensor locations is a fundamental task. There are several common strategies to address this task, for example, geometry or disk models, placing sensors at the points of highest entropy (variance) in the GP model, and A-, D-, or E-optimal design. In this paper, we tackle the combinatorial optimization problem of maximizing the mutual information between the chosen locations and the locations which are not selected. We prove that the problem of finding the configuration that maximizes mutual information is NP-complete. To address this issue, we describe a polynomial-time approximation that is within (1-1/e) of the optimum by exploiting the submodularity of mutual information. We also show how submodularity can be used to obtain online bounds, and design branch and bound search procedures. We then extend our algorithm to exploit lazy evaluations and local structure in the GP, yielding significant speedups. We also extend our approach to find placements which are robust against node failures and uncertainties in the model. These extensions are again associated with rigorous theoretical approximation guarantees, exploiting the submodularity of the objective function. We demonstrate the advantages of our approach towards optimizing mutual information in a very extensive empirical study on two real-world data sets.},
	urldate = {2017-11-17},
	journal = {J. Mach. Learn. Res.},
	author = {Krause, Andreas and Singh, Ajit and Guestrin, Carlos},
	month = jun,
	year = {2008},
	pages = {235--284}
}

@article{strobl_introduction_2009,
	title = {An {Introduction} to {Recursive} {Partitioning}: {Rationale}, {Application} and {Characteristics} of {Classification} and {Regression} {Trees}, {Bagging} and {Random} {Forests}},
	volume = {14},
	issn = {1082-989X},
	shorttitle = {An {Introduction} to {Recursive} {Partitioning}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927982/},
	doi = {10.1037/a0016973},
	abstract = {Recursive partitioning methods have become popular and widely used tools for non-parametric regression and classification in many scientific fields. Especially random forests, that can deal with large numbers of predictor variables even in the presence of complex interactions, have been applied successfully in genetics, clinical medicine and bioinformatics within the past few years., High dimensional problems are common not only in genetics, but also in some areas of psychological research, where only few subjects can be measured due to time or cost constraints, yet a large amount of data is generated for each subject. Random forests have been shown to achieve a high prediction accuracy in such applications, and provide descriptive variable importance measures reflecting the impact of each variable in both main effects and interactions., The aim of this work is to introduce the principles of the standard recursive partitioning methods as well as recent methodological improvements, to illustrate their usage for low and high dimensional data exploration, but also to point out limitations of the methods and potential pitfalls in their practical application., Application of the methods is illustrated using freely available implementations in the R system for statistical computing.},
	number = {4},
	urldate = {2017-11-17},
	journal = {Psychol Methods},
	author = {Strobl, Carolin and Malley, James and Tutz, Gerhard},
	month = dec,
	year = {2009},
	pmid = {19968396},
	pmcid = {PMC2927982},
	pages = {323--348},
	file = {PubMed Central Full Text PDF:/Users/davidfarrugia/Zotero/storage/P6BBSW4N/Strobl et al. - 2009 - An Introduction to Recursive Partitioning Rationa.pdf:application/pdf}
}

@book{zhang_recursive_2011,
	title = {Recursive {Partitioning} and {Applications}},
	url = {//www.springer.com/gp/book/9781441968234},
	abstract = {The routes to many important outcomes including diseases and ultimately death as well as financial credit consist of multiple complex pathways containing...},
	urldate = {2017-11-17},
	author = {Zhang, Heping and Singer, Burton},
	year = {2011},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/IKS7DAM6/9781441968234.html:text/html}
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures} (with comments and a rejoinder by the author)},
	volume = {16},
	issn = {0883-4237, 2168-8745},
	shorttitle = {Statistical {Modeling}},
	url = {https://projecteuclid.org/euclid.ss/1009213726},
	doi = {10.1214/ss/1009213726},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	number = {3},
	urldate = {2017-11-17},
	journal = {Statist. Sci.},
	author = {Breiman, Leo},
	month = aug,
	year = {2001},
	mrnumber = {MR1874152},
	zmnumber = {1059.62505},
	pages = {199--231},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/NNVX96HF/1009213726.html:text/html}
}

@article{welte_gambling_nodate,
	title = {Gambling participation and pathology in the {United} {States}—{A} sociodemographic analysis using classification trees},
	volume = {29},
	issn = {0306-4603},
	url = {http://www.academia.edu/27870551/Gambling_participation_and_pathology_in_the_United_States_A_sociodemographic_analysis_using_classification_trees},
	abstract = {Gambling participation and pathology in the United States—A sociodemographic analysis using classification trees},
	number = {5},
	urldate = {2017-11-17},
	journal = {Addictive Behaviors},
	author = {Welte, John W. and Barnes, Grace M. and Wieczorek, William F. and Tidwell, Marie-Cecile},
	year = {2004},
	pages = {983--989},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/543QNF5G/Gambling_participation_and_pathology_in_the_United_States_A_sociodemographic_analysis_using_cla.html:text/html}
}

@article{luellen_propensity_2005,
	title = {Propensity scores: an introduction and experimental test},
	volume = {29},
	issn = {0193-841X},
	shorttitle = {Propensity scores},
	doi = {10.1177/0193841X05275596},
	abstract = {Propensity score analysis is a relatively recent statistical innovation that is useful in the analysis of data from quasi-experiments. The goal of propensity score analysis is to balance two non-equivalent groups on observed covariates to get more accurate estimates of the effects of a treatment on which the two groups differ. This article presents a general introduction to propensity score analysis, provides an example using data from a quasi-experiment compared to a benchmark randomized experiment, offers practical advice about how to do such analyses, and discusses some limitations of the approach. It also presents the first detailed instructions to appear in the literature on how to use classification tree analysis and bagging for classification trees in the construction of propensity scores. The latter two examples serve as an introduction for researchers interested in computing propensity scores using more complex classification algorithms known as ensemble methods.},
	language = {eng},
	number = {6},
	journal = {Eval Rev},
	author = {Luellen, Jason K. and Shadish, William R. and Clark, M. H.},
	month = dec,
	year = {2005},
	pmid = {16244051},
	keywords = {Humans, Algorithms, Bias (Epidemiology), Models, Statistical, Outcome Assessment (Health Care), Randomized Controlled Trials as Topic, Research Design},
	pages = {530--558}
}

@article{berk_introduction_2006,
	title = {An {Introduction} to {Ensemble} {Methods} for {Data} {Analysis}},
	volume = {34},
	issn = {0049-1241},
	url = {https://doi.org/10.1177/0049124105283119},
	doi = {10.1177/0049124105283119},
	abstract = {This article provides an introduction to ensemble statistical procedures as a special case of algorithmic methods. The discussion begins with classification and regression trees (CART) as a didactic device to introduce many of the key issues. Following the material on CART is a consideration of cross-validation, bagging, random forests, and boosting. Major points are illustrated with analyses of real data.},
	language = {en},
	number = {3},
	urldate = {2017-11-17},
	journal = {Sociological Methods \& Research},
	author = {Berk, Richard A.},
	month = feb,
	year = {2006},
	pages = {263--295}
}

@article{baca-garcia_variables_nodate,
	title = {Variables associated with familial suicide attempts in a sample of suicide attempters},
	volume = {31},
	url = {http://www.academia.edu/13916041/Variables_associated_with_familial_suicide_attempts_in_a_sample_of_suicide_attempters},
	doi = {https://doi.org/10.1016/j.pnpbp.2007.05.019},
	abstract = {Variables associated with familial suicide attempts in a sample of suicide attempters},
	urldate = {2017-11-17},
	journal = {Progress in Neuro-Psychopharmacology and Biological Psychiatry},
	author = {Baca-Garcia, Enrique and Perez-Rodriguez, M. Mercedes and Saiz-Gonzalez, Dolores and Basurte-Villamor, Ignacio and Saiz-Ruiz, Jeronimo and Leiva-Murillo, José M. and de Prado-Cumplido, Mario and Santiago-Mozos, Ricardo and Artés-Rodríguez, Antonio and de Leon, Jose},
	year = {2007},
	pages = {1312--1316},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/JT7GTD95/Variables_associated_with_familial_suicide_attempts_in_a_sample_of_suicide_attempters.html:text/html}
}

@article{grandvalet_bagging_2004,
	title = {Bagging {Equalizes} {Influence}},
	volume = {55},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/B:MACH.0000027783.34431.42},
	doi = {10.1023/B:MACH.0000027783.34431.42},
	abstract = {Bagging constructs an estimator by averaging predictors trained on bootstrap samples. Bagged estimates almost consistently improve on the original predictor. It is thus important to understand the reasons for this success, and also for the occasional failures. It is widely believed that bagging is effective thanks to the variance reduction stemming from averaging predictors. However, seven years from its introduction, bagging is still not fully understood. This paper provides experimental evidence supporting the hypothesis that bagging stabilizes prediction by equalizing the influence of training examples. This effect is detailed in two different frameworks: estimation on the real line and regression. Bagging’s improvements/deteriorations are explained by the goodness/badness of highly influential examples, in situations where the usual variance reduction argument is at best questionable. Finally, reasons for the equalization effect are advanced. They support that other resampling strategies such as half-sampling should provide qualitatively identical effects while being computationally less demanding than bootstrap sampling.},
	language = {en},
	number = {3},
	urldate = {2017-11-17},
	journal = {Mach Learn},
	author = {Grandvalet, Yves},
	month = jun,
	year = {2004},
	pages = {251--270},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/T97PC9BJ/Grandvalet - 2004 - Bagging Equalizes Influence.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/9BBW2EEP/bmach.0000027783.34431.html:text/html}
}

@article{strobl_bias_2007,
	title = {Bias in random forest variable importance measures: {Illustrations}, sources and a solution},
	volume = {8},
	issn = {1471-2105},
	shorttitle = {Bias in random forest variable importance measures},
	url = {https://doi.org/10.1186/1471-2105-8-25},
	doi = {10.1186/1471-2105-8-25},
	abstract = {Variable importance measures for random forests have been receiving increased attention as a means of variable selection in many classification tasks in bioinformatics and related scientific fields, for instance to select a subset of genetic markers relevant for the prediction of a certain disease. We show that random forest variable importance measures are a sensible means for variable selection in many applications, but are not reliable in situations where potential predictor variables vary in their scale of measurement or their number of categories. This is particularly important in genomics and computational biology, where predictors often include variables of different types, for example when predictors include both sequence data and continuous variables such as folding energy, or when amino acid sequence data show different numbers of categories.},
	urldate = {2017-11-17},
	journal = {BMC Bioinformatics},
	author = {Strobl, Carolin and Boulesteix, Anne-Laure and Zeileis, Achim and Hothorn, Torsten},
	month = jan,
	year = {2007},
	pages = {25},
	annote = {Pages 25 in PDF},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/8DBVNZQB/Strobl et al. - 2007 - Bias in random forest variable importance measures.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/VZJ5B6HJ/1471-2105-8-25.html:text/html}
}

@article{segal_machine_2003,
	title = {Machine {Learning} {Benchmarks} and {Random} {Forest} {Regression}},
	abstract = {Breiman (2001a,b) has recently developed an ensemble classification and regression approach that displayed outstanding performance with regard prediction error on a suite of benchmark datasets. As the base constituents of the ensemble are tree-structured predictors, and since each of these is constructed using an injection of randomness, the method is called 'random forests'. That the exceptional performance is attained with seemingly only a single tuning parameter, to which sensitivity is minimal, makes the methodology all the more remarkable. The individual trees comprising the forest are all grown to maximal depth. While this helps with regard bias, there is the familiar tradeoff with variance. However, these variability concerns were potentially obscured because of an interesting feature of those benchmarking datasets extracted from the UCI machine learning repository for testing: all these datasets are hard to overfit using tree-structured methods. This raises issues about the scope of the repository. With this as motivation, and coupled with experience from boosting methods, we revisit the formulation of random forests and investigate prediction performance on real-world and simulated datasets for which maximally sized trees do overfit. These explorations reveal that gains can be realized by additional tuning to regulate tree size via limiting the number of splits and/or the size of nodes for which splitting is allowed. Nonetheless, even in these settings, good performance for random forests can be attained by using larger (than default) primary tuning parameter values. Abstract. Breiman (2001a,b) has recently developed an ensemble classification and regression approach that displayed outstanding performance with regard prediction error on a suite of benchmark datasets. As the base constituents of the ensemble are tree-structured predictors, and since each of these is constructed using an injection of randomness, the method is called 'random forests'. That the exceptional performance is attained with seemingly only a single tuning pa-rameter, to which sensitivity is minimal, makes the methodology all the more remarkable. The individual trees comprising the forest are all grown to maximal depth. While this helps with regard bias, there is the familiar tradeoff with variance. However, these variability concerns were potentially obscured because of an interesting feature of those benchmarking datasets extracted from the UCI machine learning repository for testing: all these datasets are hard to overfit using tree-structured methods. This raises issues about the scope of the repository. With this as motivation, and coupled with experience from boosting methods, we revisit the formulation of random forests and investigate prediction performance on real-world and simu-lated datasets for which maximally sized trees do overfit. These explorations reveal that gains can be realized by additional tuning to regulate tree size via limiting the number of splits and/or the size of nodes for which splitting is allowed. Nonetheless, even in these settings, good performance for random forests can be attained by using larger (than default) primary tuning parameter values.},
	journal = {Technical Report, Center for Bioinformatics \& Molecular Biostatistics, University of California, San Francisco},
	author = {Segal, Mark},
	month = may,
	year = {2003},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/E36XS2W4/Segal - 2003 - Machine Learning Benchmarks and Random Forest Regr.pdf:application/pdf}
}

@article{heidema_challenge_2006,
	title = {The challenge for genetic epidemiologists: how to analyze large numbers of {SNPs} in relation to complex diseases},
	volume = {7},
	shorttitle = {The challenge for genetic epidemiologists},
	doi = {10.1186/1471.1-2156-7-23},
	abstract = {Genetic epidemiologists have taken the challenge to identify genetic polymorphisms involved in the development of diseases. Many have collected data on large numbers of genetic markers but are not familiar with available methods to assess their association with complex diseases. Statistical methods have been developed for analyzing the relation between large numbers of genetic and environmental predictors to disease or disease-related variables in genetic association studies. In this commentary we discuss logistic regression analysis, neural networks, including the parameter decreasing method (PDM) and genetic programming optimized neural networks (GPNN) and several non-parametric methods, which include the set association approach, combinatorial partitioning method (CPM), restricted partitioning method (RPM), multifactor dimensionality reduction (MDR) method and the random forests approach. The relative strengths and weaknesses of these methods are highlighted. Logistic regression and neural networks can handle only a limited number of predictor variables, depending on the number of observations in the dataset. Therefore, they are less useful than the non-parametric methods to approach association studies with large numbers of predictor variables. GPNN on the other hand may be a useful approach to select and model important predictors, but its performance to select the important effects in the presence of large numbers of predictors needs to be examined. Both the set association approach and random forests approach are able to handle a large number of predictors and are useful in reducing these predictors to a subset of predictors with an important contribution to disease. The combinatorial methods give more insight in combination patterns for sets of genetic and/or environmental predictor variables that may be related to the outcome variable. As the non-parametric methods have different strengths and weaknesses we conclude that to approach genetic association studies using the case-control design, the application of a combination of several methods, including the set association approach, MDR and the random forests approach, will likely be a useful strategy to find the important genes and interaction patterns involved in complex diseases.},
	journal = {BMC Genetics},
	author = {Heidema, AG and Boer, JM and Nagelkerke, N and Mariman, Edwin and van der A, DL and Feskens, EJM},
	month = apr,
	year = {2006}
}

@inproceedings{caruana_empirical_2006,
	address = {New York, NY, USA},
	series = {{ICML} '06},
	title = {An {Empirical} {Comparison} of {Supervised} {Learning} {Algorithms}},
	isbn = {978-1-59593-383-6},
	url = {http://doi.acm.org/10.1145/1143844.1143865},
	doi = {10.1145/1143844.1143865},
	abstract = {A number of supervised learning methods have been introduced in the last decade. Unfortunately, the last comprehensive empirical evaluation of supervised learning was the Statlog Project in the early 90's. We present a large-scale empirical comparison between ten supervised learning methods: SVMs, neural nets, logistic regression, naive bayes, memory-based learning, random forests, decision trees, bagged trees, boosted trees, and boosted stumps. We also examine the effect that calibrating the models via Platt Scaling and Isotonic Regression has on their performance. An important aspect of our study is the use of a variety of performance criteria to evaluate the learning methods.},
	urldate = {2017-11-17},
	booktitle = {Proceedings of the 23rd {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Caruana, Rich and Niculescu-Mizil, Alexandru},
	year = {2006},
	pages = {161--168}
}

@misc{woodie_using_2016,
	title = {Using {Big} {Data} {Analytics} to {Fight} {Gambling} {Addiction}},
	url = {https://www.datanami.com/2016/03/10/using-big-data-analytics-to-fight-gambling-addiction/},
	abstract = {It's estimated that 3 to 5 percent of people who gamble develop an addiction to the activity, which can lead to an array of problems for gamblers, their fa},
	urldate = {2017-11-18},
	journal = {Datanami},
	author = {Woodie, Alex},
	month = mar,
	year = {2016},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/QDF2MBJ3/using-big-data-analytics-to-fight-gambling-addiction.html:text/html}
}

@article{percy_predicting_2016,
	title = {Predicting online gambling self-exclusion: an analysis of the performance of supervised machine learning models},
	volume = {16},
	issn = {1445-9795},
	shorttitle = {Predicting online gambling self-exclusion},
	url = {http://dx.doi.org/10.1080/14459795.2016.1151913},
	doi = {10.1080/14459795.2016.1151913},
	abstract = {As gambling operators become increasingly sophisticated in their analysis of individual gambling behaviour, this study evaluates the potential for using machine learning techniques to identify individuals who used self-exclusion tools out of a sample of 845 online gamblers, based on analysing trends in their gambling behaviour. Being able to identify other gamblers whose behaviour is similar to those who decided to use self-exclusion tools could, for instance, be used to share responsible gaming messages or other information that aids self-aware gambling and reduces the risk of adverse outcomes. However, operators need to understand how accurate models can be and which techniques work well. The purpose of the article is to identify the most accurate technique out of four highly diverse techniques and to discuss how to deal analytically and practically with a rare event like self-exclusion, which was used by fewer than 1\% of gamblers in our data-set. We conclude that balanced training data-sets are necessary for creating effective models and that, on our data-set, the most effective method is the random forest technique which achieves an accuracy improvement of 35 percentage points versus baseline estimates.},
	number = {2},
	urldate = {2017-11-18},
	journal = {International Gambling Studies},
	author = {Percy, Christian and França, Manoel and Dragičević, Simo and Garcez, Artur d’Avila},
	month = may,
	year = {2016},
	keywords = {responsible gambling, gambling self-exclusion, machine learning, supervised learning algorithms, problem gambling, Internet gambling, three-tier model},
	pages = {193--210},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/H86R3HU9/14459795.2016.html:text/html}
}

@article{hayer_self-exclusion_2011,
	title = {Self-exclusion as a harm minimization strategy: evidence for the casino sector from selected {European} countries},
	volume = {27},
	issn = {1573-3602},
	shorttitle = {Self-exclusion as a harm minimization strategy},
	doi = {10.1007/s10899-010-9227-8},
	abstract = {As the international gambling market continues to expand, determining effective approaches to prevent gambling-related problems becomes increasingly important. Despite a lack of in-depth research into its benefits, self-exclusion is one such measure already in use around the world in various sectors of the gambling industry. The present study is the first of its kind to examine the effectiveness of self-exclusion schemes in the casino sector in selected European countries. A written survey yielded a sample of N=152 (self)-excluded gamblers. In addition to this cross-section analysis, a small sub-group (n=31) was monitored over time by means of follow-up surveys carried out 1, 6, and 12month(s) after the exclusion agreement came into force. The results reveal that the self-excluded individuals are typically under a great deal of strain and show a relatively pronounced willingness to change. However, this largely reaches its peak at the time the decision to self-exclude is made. From a longitudinal perspective, various parameters indicate a clear improvement in psychosocial functioning; a favorable effect that also starts directly after the exclusion agreement was signed. Finally, considering theoretical and empirical findings, possibilities for optimizing (self-)exclusion schemes will be discussed.},
	language = {eng},
	number = {4},
	journal = {J Gambl Stud},
	author = {Hayer, Tobias and Meyer, Gerhard},
	month = dec,
	year = {2011},
	pmid = {21132355},
	keywords = {Behavior, Addictive, Gambling, Humans, Self Efficacy, Adult, Female, Male, Behavior Control, Europe, Middle Aged, Patient Acceptance of Health Care, Patient Compliance, Personal Autonomy, Severity of Illness Index, Treatment Outcome},
	pages = {685--700}
}

@inproceedings{wardle_understanding_2012,
	address = {London},
	title = {Understanding self-exclusion – profile, processes and improvements: {Evidence} and implications from a research study of online betting exchange users},
	url = {http://www.responsiblegambling.org/docs/discovery-2012/understanding-self-exclusion-profile-processes-and-improvements-evidence-and-implications-from-a-research-study-of-online-betting-exchange-users.pdf?sfvrsn=12},
	publisher = {NatCen Social Research},
	author = {Wardle, Heather},
	year = {2012}
}

@misc{power_barred_2016,
	title = {Barred {Online} {Gambling} {App} {Self} {Exclusion} {Tool}},
	url = {https://play.google.com/store/apps/details?id=com.appmakr.onlinegamblingaddiction},
	abstract = {Gambling is a physical addiction and gambling can be prevented with self exclusion. Gambling destroys lives, gambling addiction causes financial problems, a gambling problem can destroy your life. If gambling is taking over your life, If you have problems, use this app, it is free. It provides information on how to make physical barriers to stop gambling, including getting barred from gambling websites using the apps self exclusion system by contacting over 425 gambling websites to request self exclusion using a medical reason which cannot be undone. This is the first ever system to be developed online.Learn about which sites are corrupt and under unregulated gambling networks, which sites are blacklisted... to avoid at all costs, fake Russian sites... hidden terms and conditions, sites that don't pay out, cheating sites.... Contact support networks online for help, to the main site www.onlinegamblingaddiction.net for help to quit this addiction. Learn to stop gambling and help others avoid gambling online. We would like to hear especially from underage gamblers, we send information to gambling authorities to try to help improve regulation. We help users get barred and self excluded permanently from websites... bookies, casinos, bingo, slots, poker... this App is promoting getting medically barred for those who are medically suffering from gambling.Eventually there will come a time when gambling regulation will be universal and authorities will have a simple solution to get barred gambling online... this is what should be available for those who are genuinely ill. This is what we promote by helping you to get barred online from gambling and help create physical barriers and limit access to funds and means to gamble.If you have a family member or spouse with a gambling problem please also get in touch for help and support.},
	urldate = {2017-11-18},
	publisher = {Higher Power},
	author = {Power, Higher},
	month = apr,
	year = {2016},
	file = {App Screenshot:/Users/davidfarrugia/Zotero/storage/SJHQUNHV/O7akKJ4c0kEl0sFUpS0KmUsrGyae1ZCVEVpuUATlX-sh_Vy1_dI5VFIQLWr-oamaADNk=h310.png:image/png;App Screenshot:/Users/davidfarrugia/Zotero/storage/JCMC7XGD/YaEq0iWXthJqfxmERa25OfP5QrEqtMyMAUfPd0xOQh-EsFCg75LOI1_230kzINrbBI8=h310.png:image/png;App Screenshot:/Users/davidfarrugia/Zotero/storage/48DBHHFW/wQ22_HgG6yvRMJVUwdVwaiGm30v7a__dVzDi5-0f_YGUXo-2a9fYMwBT75KOlWoi3-Mm=h310.jpg:image/jpeg;App Screenshot:/Users/davidfarrugia/Zotero/storage/GDFTI5DD/Tw4Tx33T1oWBl1HiWQQ5ewKtWXMxUDKdLl-t6xzRuxXgxmKD3BD2IviIs-KhEhp6M87v=h310.png:image/png;App Screenshot:/Users/davidfarrugia/Zotero/storage/2GB6W5H4/BTOhes_Nnl8vcAixufjl-pigX0ZqKVUg5TUeF_JqF2vbXAp2-jabDnFywkyShKv4SQ=h310.png:image/png}
}

@misc{noauthor_global_nodate,
	title = {The {Global} {Games} {Market} 2017 {\textbar} {Per} {Region} \& {Segment}},
	url = {https://newzoo.com/insights/articles/the-global-games-market-will-reach-108-9-billion-in-2017-with-mobile-taking-42/},
	abstract = {Global Games Market 2017: Revenues will reach \$108.9Bn in 2017 with the mobile games market worth \$46.1 billion. Digital games will account for \$94.4Bn.},
	urldate = {2017-11-18},
	journal = {Newzoo},
	author = {McDonald, Emma},
	month = apr,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/X87FQ5Z9/the-global-games-market-will-reach-108-9-billion-in-2017-with-mobile-taking-42.html:text/html}
}

@misc{markets_global_nodate,
	title = {Global \$81.7 {Bn} {Online} {Gambling} {Market} to 2022 {By} {Type}, {Device}, {Region}},
	url = {https://www.prnewswire.com/news-releases/global-817-bn-online-gambling-market-to-2022-by-type-device-region-300520256.html},
	abstract = {DUBLIN, Sept. 15, 2017 /PRNewswire/ --{\textless}br /{\textgreater}{\textless}br /{\textgreater}The "Global Online Gambling Market - By Type, Device, Region - Market Size,...},
	urldate = {2017-11-18},
	author = {{Research and Markets}},
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/ZARHCR2P/global-817-bn-online-gambling-market-to-2022-by-type-device-region-300520256.html:text/html}
}

@misc{team_why_2017,
	title = {Why {Online} {Gambling} and {Gaming} {Companies} are {Rolling} the {Dice} on {Machine} {Learning}},
	url = {https://insidebigdata.com/2017/01/28/online-gambling-gaming-companies-rolling-dice-machine-learning/},
	abstract = {In this special guest feature, Jay Patani is a Tech Evangelist for ITRS Group, examines how online gambling and gaming companies have access to a wealth of big data created by every click of a customer’s mouse. The key to extracting valuable predictive insights from that data will be sophisticated machine learning.},
	urldate = {2017-11-18},
	journal = {insideBIGDATA},
	author = {Team, Editorial},
	month = jan,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/VM9XZ53K/online-gambling-gaming-companies-rolling-dice-machine-learning.html:text/html}
}

@misc{epsrc_online_2015,
	title = {Online gambling to get safer through better prediction of addiction},
	url = {https://www.sciencedaily.com/releases/2015/10/151023084506.htm},
	abstract = {A new ‘early warning’ system that automatically informs gamblers as soon as their behavior shows signs of turning into an addiction is helping people engage in the pastime responsibly.},
	urldate = {2017-11-18},
	journal = {ScienceDaily},
	author = {{EPSRC}},
	year = {2015},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/ATCNS3ER/151023084506.html:text/html}
}

@article{breiman_random_2001,
	title = {Random {Forests}},
	volume = {45},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/A:1010933404324},
	doi = {10.1023/A:1010933404324},
	abstract = {Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund \& R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.},
	language = {en},
	number = {1},
	urldate = {2017-11-18},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = oct,
	year = {2001},
	pages = {5--32},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/P5ZZ9D4B/Breiman - 2001 - Random Forests.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/6IKFZ87J/A1010933404324.html:text/html}
}

@article{tang_identification_2009,
	title = {Identification of genes and haplotypes that predict rheumatoid arthritis using random forests},
	volume = {3 Suppl 7},
	doi = {10.1186/1753-6561-3-s7-s68},
	abstract = {ABSTRACT : Random forest (RF) analysis of genetic data does not require specification of the mode of inheritance, and provides measures of variable importance that incorporate interaction effects. In this paper we describe RF-based approaches for assessment of gene and haplotype importance, and apply these approaches to a subset of the North American Rheumatoid Arthritis Consortium case-control data provided by Genetic Analysis Workshop 16. The RF analyses of 37 genes identified many of the same genes as logistic regression, but also suggested importance of certain single-nucleotide polymorphism and genes that were not ranked highly by logistic regression. A new permutation method did not reveal strong evidence of gene-gene interaction effects in these data. Although RFs are a promising approach for genetic data analysis, extensions beyond simple single-nucleotide polymorphism analyses and modifications to improve computational feasibility are needed.},
	journal = {BMC proceedings},
	author = {Tang, Rui and Sinnwell, Jason and Li, Jia and N Rider, David and Andrade, Mariza and Biernacka, Joanna},
	month = dec,
	year = {2009},
	pages = {S68},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/5V6B284X/Tang et al. - 2009 - Identification of genes and haplotypes that predic.pdf:application/pdf}
}

@inproceedings{svetnik_application_2004,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Application of {Breiman}’s {Random} {Forest} to {Modeling} {Structure}-{Activity} {Relationships} of {Pharmaceutical} {Molecules}},
	isbn = {978-3-540-22144-9 978-3-540-25966-4},
	url = {https://link.springer.com/chapter/10.1007/978-3-540-25966-4_33},
	doi = {10.1007/978-3-540-25966-4_33},
	abstract = {Leo Breiman’s Random Forest ensemble learning procedure is applied to the problem of Quantitative Structure-Activity Relationship (QSAR) modeling for pharmaceutical molecules. This entails using a quantitative description of a compound’s molecular structure to predict that compound’s biological activity as measured in an in vitro assay. Without any parameter tuning, the performance of Random Forest with default settings on six publicly available data sets is already as good or better than that of three other prominent QSAR methods: Decision Tree, Partial Least Squares, and Support Vector Machine. In addition to reliable prediction accuracy, Random Forest provides variable importance measures which can be used in a variable reduction wrapper algorithm. Comparisons of various such wrappers and between Random Forest and Bagging are presented.},
	language = {en},
	urldate = {2017-11-19},
	booktitle = {Multiple {Classifier} {Systems}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Svetnik, Vladimir and Liaw, Andy and Tong, Christopher and Wang, Ting},
	month = jun,
	year = {2004},
	pages = {334--343},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/3F89L8ZE/978-3-540-25966-4_33.html:text/html}
}

@misc{hardesty_explained:_2017,
	title = {Explained: {Neural} networks},
	shorttitle = {Explained},
	url = {http://news.mit.edu/2017/explained-neural-networks-deep-learning-0414},
	abstract = {Ballyhooed artificial-intelligence technique known as “deep learning” revives 70-year-old idea.},
	urldate = {2017-11-19},
	journal = {MIT News},
	author = {Hardesty, Larry},
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/DLYRYLZY/explained-neural-networks-deep-learning-0414.html:text/html}
}

@misc{xenonstack_overview_2017,
	title = {Overview of {Artificial} {Neural} {Networks} and its {Applications}},
	url = {https://hackernoon.com/overview-of-artificial-neural-networks-and-its-applications-2525c1addff7},
	abstract = {What is Neural Network?},
	urldate = {2017-11-19},
	journal = {Hacker Noon},
	author = {XenonStack},
	month = jul,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/3WTK64XB/overview-of-artificial-neural-networks-and-its-applications-2525c1addff7.html:text/html}
}

@article{mair_investigation_2000,
	title = {An investigation of machine learning based prediction systems},
	volume = {53},
	issn = {0164-1212},
	url = {http://www.sciencedirect.com/science/article/pii/S0164121200000054},
	doi = {10.1016/S0164-1212(00)00005-4},
	abstract = {Traditionally, researchers have used either off-the-shelf models such as COCOMO, or developed local models using statistical techniques such as stepwise regression, to obtain software effort estimates. More recently, attention has turned to a variety of machine learning methods such as artificial neural networks (ANNs), case-based reasoning (CBR) and rule induction (RI). This paper outlines some comparative research into the use of these three machine learning methods to build software effort prediction systems. We briefly describe each method and then apply the techniques to a dataset of 81 software projects derived from a Canadian software house in the late 1980s. We compare the prediction systems in terms of three factors: accuracy, explanatory value and configurability. We show that ANN methods have superior accuracy and that RI methods are least accurate. However, this view is somewhat counteracted by problems with explanatory value and configurability. For example, we found that considerable effort was required to configure the ANN and that this compared very unfavourably with the other techniques, particularly CBR and least squares regression (LSR). We suggest that further work be carried out, both to further explore interaction between the end-user and the prediction system, and also to facilitate configuration, particularly of ANNs.},
	number = {1},
	urldate = {2017-11-19},
	journal = {Journal of Systems and Software},
	author = {Mair, Carolyn and Kadoda, Gada and Lefley, Martin and Phalp, Keith and Schofield, Chris and Shepperd, Martin and Webster, Steve},
	month = jul,
	year = {2000},
	keywords = {Case-based reasoning, Machine learning, Neural net, Prediction system, Rule induction, Software cost model, Software effort estimation},
	pages = {23--29},
	file = {ScienceDirect Snapshot:/Users/davidfarrugia/Zotero/storage/C7B2WIQD/S0164121200000054.html:text/html}
}

@article{tollenaar_which_2013,
	title = {Which method predicts recidivism best?: a comparison of statistical, machine learning and data mining predictive models},
	volume = {176},
	issn = {1467-985X},
	shorttitle = {Which method predicts recidivism best?},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/j.1467-985X.2012.01056.x/abstract},
	doi = {10.1111/j.1467-985X.2012.01056.x},
	abstract = {Summary.  Using criminal population conviction histories of recent offenders, prediction mod els are developed that predict three types of criminal recidivism: general recidivism, violent recidivism and sexual recidivism. The research question is whether prediction techniques from modern statistics, data mining and machine learning provide an improvement in predictive performance over classical statistical methods, namely logistic regression and linear discrim inant analysis. These models are compared on a large selection of performance measures. Results indicate that classical methods do equally well as or better than their modern counterparts. The predictive performance of the various techniques differs only slightly for general and violent recidivism, whereas differences are larger for sexual recidivism. For the general and violent recidivism data we present the results of logistic regression and for sexual recidivism of linear discriminant analysis.},
	language = {en},
	number = {2},
	urldate = {2017-11-19},
	journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
	author = {Tollenaar, N. and van der Heijden, P. G. M.},
	month = feb,
	year = {2013},
	keywords = {Data mining, Machine learning, Linear discriminant analysis, Logistic regression, Prediction, Predictive performance, Recidivism},
	pages = {565--584},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/QZZ42PM3/abstract.html:text/html}
}

@article{likas_global_2003,
	series = {Biometrics},
	title = {The global k-means clustering algorithm},
	volume = {36},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320302000602},
	doi = {10.1016/S0031-3203(02)00060-2},
	abstract = {We present the global k-means algorithm which is an incremental approach to clustering that dynamically adds one cluster center at a time through a deterministic global search procedure consisting of N (with N being the size of the data set) executions of the k-means algorithm from suitable initial positions. We also propose modifications of the method to reduce the computational load without significantly affecting solution quality. The proposed clustering methods are tested on well-known data sets and they compare favorably to the k-means algorithm with random restarts.},
	number = {2},
	urldate = {2017-11-19},
	journal = {Pattern Recognition},
	author = {Likas, Aristidis and Vlassis, Nikos and J. Verbeek, Jakob},
	month = feb,
	year = {2003},
	keywords = {Data mining, - Trees, -Means algorithm, Clustering, Global optimization},
	pages = {451--461},
	file = {ScienceDirect Snapshot:/Users/davidfarrugia/Zotero/storage/HJZ5LZ8J/S0031320302000602.html:text/html}
}

@article{lloyd_least_1982,
	title = {Least squares quantization in {PCM}},
	volume = {28},
	issn = {0018-9448},
	doi = {10.1109/TIT.1982.1056489},
	abstract = {It has long been realized that in pulse-code modulation (PCM), with a given ensemble of signals to handle, the quantum values should be spaced more closely in the voltage regions where the signal amplitude is more likely to fall. It has been shown by Panter and Dite that, in the limit as the number of quanta becomes infinite, the asymptotic fractional density of quanta per unit voltage should vary as the one-third power of the probability density per unit voltage of signal amplitudes. In this paper the corresponding result for any finite number of quanta is derived; that is, necessary conditions are found that the quanta and associated quantization intervals of an optimum finite quantization scheme must satisfy. The optimization criterion used is that the average quantization noise power be a minimum. It is shown that the result obtained here goes over into the Panter and Dite result as the number of quanta become large. The optimum quautization schemes for2{\textasciicircum}bquanta,b=1,2, cdots, 7, are given numerically for Gaussian and for Laplacian distribution of signal amplitudes.},
	number = {2},
	journal = {IEEE Transactions on Information Theory},
	author = {Lloyd, S.},
	month = mar,
	year = {1982},
	keywords = {Least-squares approximation, PCM communication, Quantization (signal), Signal quantization},
	pages = {129--137},
	file = {IEEE Xplore Abstract Record:/Users/davidfarrugia/Zotero/storage/HY8PSRN7/1056489.html:text/html}
}

@book{nazeer_improving_2009,
	title = {Improving the {Accuracy} and {Efficiency} of the k-means {Clustering} {Algorithm}},
	volume = {2176},
	abstract = {Emergence of modern techniques for scientific data collection has resulted in large scale accumulation of data per- taining to diverse fields. Conventional database querying methods are inadequate to extract useful information from huge data banks. Cluster analysis is one of the major data analysis methods and the k-means clustering algorithm is widely used for many practical applications. But the original k-means algorithm is computationally expensive and the quality of the resulting clusters heavily depends on the selection of initial centroids. Several methods have been proposed in the literature for improving the performance of the k-means clustering algorithm. This paper proposes a method for making the algorithm more effective and efficient, so as to get better clustering with reduced complexity.},
	author = {Nazeer, K. A and Sebastian, M},
	month = jul,
	year = {2009}
}

@article{hawkins_recursive_2009,
	title = {Recursive partitioning},
	volume = {1},
	issn = {1939-0068},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/wics.44/abstract},
	doi = {10.1002/wics.44},
	abstract = {Recursive partitioning (RP) is a predictive approach with minimal statistical or model assumptions, which models the relationship in terms of trees or dendrograms. It is particularly appropriate for initial exploration of large data sets, especially messy ones, and may either validate other approaches with stronger model assumptions or lead to a final analysis in its own right. It may be used for either a nominal scale (categorical) or an interval-scale (numeric) dependent variable. A major issue is the size of tree to be fitted; different approaches for this have been proposed. Like many other feature selection methods, RP is unstable, the model being potentially sensitive to minor perturbations in the data. Fitting multiple trees helps explore alternative models, and also provides better predictions than those giving by a single tree. RP is well supported in both commercial and public domain software. Copyright © 2009 John Wiley \& Sons, Inc. For further resources related to this article, please visit the WIREs website.},
	language = {en},
	number = {3},
	urldate = {2017-11-19},
	journal = {WIREs Comp Stat},
	author = {Hawkins, Douglas M.},
	month = nov,
	year = {2009},
	keywords = {interaction, localization, predictive modeling, segmentation},
	pages = {290--295},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/E95947MY/abstract.html:text/html}
}

@article{rouzier_direct_2009,
	title = {Direct comparison of logistic regression and recursive partitioning to predict chemotherapy response of breast cancer based on clinical pathological variables},
	volume = {117},
	issn = {1573-7217},
	doi = {10.1007/s10549-009-0308-2},
	abstract = {The purpose was to compare logistic regression model (LRM) and recursive partitioning (RP) to predict pathologic complete response to preoperative chemotherapy in patients with breast cancer. The two models were built in a same training set of 496 patients and validated in a same validation set of 337 patients. Model performance was quantified with respect to discrimination (evaluated by the areas under the receiver operating characteristics curves (AUC)) and calibration. In the training set, AUC were similar for LRM and RP models (0.77 (95\% confidence interval, 0.74-0.80) and 0.75 (95\% CI, 0.74-0.79), respectively) while LRM outperformed RP in the validation set (0.78 (95\% CI, 0.74-0.82) versus 0.64 (95\% CI, 0.60-0.67). LRM model also outperformed RP model in term of calibration. In these real datasets, LRM model outperformed RP model. It is therefore more suitable for clinical use.},
	language = {eng},
	number = {2},
	journal = {Breast Cancer Res. Treat.},
	author = {Rouzier, Roman and Coutant, Charles and Lesieur, Bénédicte and Mazouni, Chafika and Incitti, Roberto and Natowicz, René and Pusztai, Lajos},
	month = sep,
	year = {2009},
	pmid = {19152025},
	keywords = {Humans, Female, Models, Statistical, Middle Aged, Treatment Outcome, Antineoplastic Agents, Area Under Curve, Breast Neoplasms, Chemotherapy, Adjuvant, Logistic Models, Neoplasm Staging, ROC Curve},
	pages = {325--331}
}

@misc{statistics_solutions_what_2017,
	title = {What is {Logistic} {Regression}?},
	url = {http://www.statisticssolutions.com/what-is-logistic-regression/},
	abstract = {Logistic regression is the appropriate regression analysis to conduct when the dependent variable is dichotomous (binary).},
	urldate = {2017-11-20},
	journal = {Statistics Solutions},
	author = {{Statistics Solutions}},
	month = jan,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/TAWAUXNE/what-is-logistic-regression.html:text/html}
}

@article{haefeli_early_2011,
	title = {Early detection items and responsible gambling features for online gambling},
	volume = {11},
	issn = {1445-9795},
	url = {http://dx.doi.org/10.1080/14459795.2011.604643},
	doi = {10.1080/14459795.2011.604643},
	abstract = {Early detection is an effective building block for the prevention of problem gambling. This study aims to identify communication-based indicators for gambling-related problems in the setting of online gambling. In the framework of a semi-structured interview, customer service employees of three online gambling operators were surveyed, to identify indicators in customer correspondence could be used as a predictor for gambling-related problems. In a confirmatory part of the study, we investigated to what degree these indicators are able to predict problem gambling in a prospective empirical design. An optimally parsimonious log-linear model, was able to correctly predict 76.6\% of the cases. Discussed in the light of this evidence, communication-based indicators could constitute an effective component of early detection. Due to the fact that the internet offers optimal conditions for consistent monitoring and objective analysis, the suggested predictive model could be combined with other models, relying on the analysis of gambling behaviour.},
	number = {3},
	urldate = {2017-11-20},
	journal = {International Gambling Studies},
	author = {Haefeli, Joerg and Lischer, Suzanne and Schwarz, Juerg},
	month = dec,
	year = {2011},
	keywords = {responsible gambling, harm-minimization, internet, predictors, screening instruments},
	pages = {273--288},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/DQZJNK4I/Haefeli et al. - 2011 - Early detection items and responsible gambling fea.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/IHRC7FFB/14459795.2011.html:text/html}
}

@article{garg_tensor-based_2014,
	title = {Tensor-based methods for handling missing data in quality-of-life questionnaires},
	volume = {18},
	issn = {2168-2208},
	doi = {10.1109/JBHI.2013.2288803},
	abstract = {A common problem with self-report quality-of-life questionnaires is missing data. Despite enormous care and effort to prevent it, some level of missing data is common and unavoidable. Missing data can have a detrimental impact on the data analysis. In this paper, a novel approach to imputing missing data in quality-of-life questionnaires is proposed, based on matrix and tensor decompositions. In order to illustrate and assess those methods, two datasets are considered: The first dataset contains the responses of 100 patients to a systemic lupus erythematosus-specific quality-of-life questionnaire; the other contains the responses of 43 patients to a rhino-conjunctivitis quality-of-life questionnaire. The two datasets contain almost no missing data, and for testing purposes, data entries are removed at random to have missing completely at random data. Several proportions of missing values are considered, and for each, the imputation error is assessed through k-fold cross validation. We also evaluate different imputation methods for missing at random and missing not at randomdata. The numerical results demonstrate that the proposed tensor factorization-based methods outperform standard methods in terms of root mean square error with at least 4\% improvement, while the bias and variance are similar.},
	language = {eng},
	number = {5},
	journal = {IEEE J Biomed Health Inform},
	author = {Garg, Lalit and Dauwels, Justin and Earnest, Arul and Leong, Khai Pang},
	month = sep,
	year = {2014},
	pmid = {24235317},
	keywords = {Humans, Surveys and Questionnaires, Algorithms, Conjunctivitis, Databases, Factual, Electronic Health Records, Lupus Erythematosus, Systemic, Medical Informatics Applications, Models, Theoretical, Quality of Life, Reproducibility of Results},
	pages = {1571--1580}
}

@article{sehgal_collateral_2005,
	title = {Collateral missing value imputation: a new robust missing value estimation algorithm for microarray data},
	volume = {21},
	issn = {1367-4803},
	shorttitle = {Collateral missing value imputation},
	doi = {10.1093/bioinformatics/bti345},
	abstract = {MOTIVATION: Microarray data are used in a range of application areas in biology, although often it contains considerable numbers of missing values. These missing values can significantly affect subsequent statistical analysis and machine learning algorithms so there is a strong motivation to estimate these values as accurately as possible before using these algorithms. While many imputation algorithms have been proposed, more robust techniques need to be developed so that further analysis of biological data can be accurately undertaken. In this paper, an innovative missing value imputation algorithm called collateral missing value estimation (CMVE) is presented which uses multiple covariance-based imputation matrices for the final prediction of missing values. The matrices are computed and optimized using least square regression and linear programming methods.
RESULTS: The new CMVE algorithm has been compared with existing estimation techniques including Bayesian principal component analysis imputation (BPCA), least square impute (LSImpute) and K-nearest neighbour (KNN). All these methods were rigorously tested to estimate missing values in three separate non-time series (ovarian cancer based) and one time series (yeast sporulation) dataset. Each method was quantitatively analyzed using the normalized root mean square (NRMS) error measure, covering a wide range of randomly introduced missing value probabilities from 0.01 to 0.2. Experiments were also undertaken on the yeast dataset, which comprised 1.7\% actual missing values, to test the hypothesis that CMVE performed better not only for randomly occurring but also for a real distribution of missing values. The results confirmed that CMVE consistently demonstrated superior and robust estimation capability of missing values compared with other methods for both series types of data, for the same order of computational complexity. A concise theoretical framework has also been formulated to validate the improved performance of the CMVE algorithm.
AVAILABILITY: The CMVE software is available upon request from the authors.},
	language = {eng},
	number = {10},
	journal = {Bioinformatics},
	author = {Sehgal, Muhammad Shoaib B. and Gondal, Iqbal and Dooley, Laurence S.},
	month = may,
	year = {2005},
	pmid = {15731210},
	keywords = {Humans, Female, Algorithms, Models, Statistical, BRCA1 Protein, BRCA2 Protein, Data Interpretation, Statistical, Gene Expression Profiling, Likelihood Functions, Models, Biological, Oligonucleotide Array Sequence Analysis, Ovarian Neoplasms, Saccharomyces cerevisiae Proteins, Sample Size},
	pages = {2417--2423}
}

@article{lemmens_bagging_2006,
	title = {Bagging and {Boosting} {Classification} {Trees} to {Predict} {Churn}},
	volume = {43},
	issn = {0022-2437},
	url = {http://journals.ama.org/doi/abs/10.1509/jmkr.43.2.276},
	doi = {10.1509/jmkr.43.2.276},
	abstract = {In this article, the authors explore the bagging and boosting classification techniques. They apply the two techniques to a customer database of an anonymous U.S. wireless telecommunications company, and both significantly improve accuracy in predicting churn. This higher predictive performance could ultimately lead to incremental profits for companies that use these methods. Furthermore, the results recommend the use of a balanced sampling scheme when predicting a rare event from large data sets, but this requires an appropriate bias correction.},
	number = {2},
	urldate = {2017-11-20},
	journal = {Journal of Marketing Research},
	author = {Lemmens, Aurélie and Croux, Christophe},
	month = may,
	year = {2006},
	pages = {276--286},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/JVNM5SFI/jmkr.43.2.html:text/html}
}

@article{chekroud_cross-trial_2016,
	title = {Cross-trial prediction of treatment outcome in depression: a machine learning approach},
	volume = {3},
	issn = {2215-0366, 2215-0374},
	shorttitle = {Cross-trial prediction of treatment outcome in depression},
	url = {http://www.thelancet.com/journals/lanpsy/article/PIIS2215-0366(15)00471-X/abstract},
	doi = {10.1016/S2215-0366(15)00471-X},
	abstract = {{\textless}h2{\textgreater}Summary{\textless}/h2{\textgreater}{\textless}h3{\textgreater}Background{\textless}/h3{\textgreater}{\textless}p{\textgreater}Antidepressant treatment efficacy is low, but might be improved by matching patients to interventions. At present, clinicians have no empirically validated mechanisms to assess whether a patient with depression will respond to a specific antidepressant. We aimed to develop an algorithm to assess whether patients will achieve symptomatic remission from a 12-week course of citalopram.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Methods{\textless}/h3{\textgreater}{\textless}p{\textgreater}We used patient-reported data from patients with depression (n=4041, with 1949 completers) from level 1 of the Sequenced Treatment Alternatives to Relieve Depression (STAR*D; ClinicalTrials.gov, number NCT00021528) to identify variables that were most predictive of treatment outcome, and used these variables to train a machine-learning model to predict clinical remission. We externally validated the model in the escitalopram treatment group (n=151) of an independent clinical trial (Combining Medications to Enhance Depression Outcomes [COMED]; ClinicalTrials.gov, number NCT00590863).{\textless}/p{\textgreater}{\textless}h3{\textgreater}Findings{\textless}/h3{\textgreater}{\textless}p{\textgreater}We identified 25 variables that were most predictive of treatment outcome from 164 patient-reportable variables, and used these to train the model. The model was internally cross-validated, and predicted outcomes in the STAR*D cohort with accuracy significantly above chance (64·6\% [SD 3·2]; p{\textless}0·0001). The model was externally validated in the escitalopram treatment group (N=151) of COMED (accuracy 59·6\%, p=0.043). The model also performed significantly above chance in a combined escitalopram-buproprion treatment group in COMED (n=134; accuracy 59·7\%, p=0·023), but not in a combined venlafaxine-mirtazapine group (n=140; accuracy 51·4\%, p=0·53), suggesting specificity of the model to underlying mechanisms.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Interpretation{\textless}/h3{\textgreater}{\textless}p{\textgreater}Building statistical models by mining existing clinical trial data can enable prospective identification of patients who are likely to respond to a specific antidepressant.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Funding{\textless}/h3{\textgreater}{\textless}p{\textgreater}Yale University.{\textless}/p{\textgreater}},
	language = {English},
	number = {3},
	urldate = {2017-11-20},
	journal = {The Lancet Psychiatry},
	author = {Chekroud, Adam Mourad and Zotti, Ryan Joseph and Shehzad, Zarrar and Gueorguieva, Ralitza and Johnson, Marcia K. and Trivedi, Madhukar H. and Cannon, Tyrone D. and Krystal, John Harrison and Corlett, Philip Robert},
	month = mar,
	year = {2016},
	pages = {243--250},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/MDXNUY6G/abstract.html:text/html}
}

@misc{says_quick_2015,
	title = {Quick {Guide} to {Boosting} {Algorithms} in {Machine} {Learning}},
	url = {https://www.analyticsvidhya.com/blog/2015/11/quick-introduction-boosting-algorithms-machine-learning/},
	abstract = {Here is a simple explanation of boosting, a machine learning ensemble algorithm to boost accuracy of predictive models with AdaBoost and GBM},
	urldate = {2017-11-20},
	journal = {Analytics Vidhya},
	author = {says, Clyde Govindsamy},
	month = nov,
	year = {2015},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/R4PIKRXL/quick-introduction-boosting-algorithms-machine-learning.html:text/html}
}

@misc{brownlee_boosting_2016,
	title = {Boosting and {AdaBoost} for {Machine} {Learning}},
	url = {https://machinelearningmastery.com/boosting-and-adaboost-for-machine-learning/},
	abstract = {Boosting is an ensemble technique that attempts to create a strong classifier from a number of weak classifiers. In this post you will discover the AdaBoost Ensemble method for machine learning. After reading this post, you will know: What the boosting ensemble method is and generally how it works. How to learn to boost decision …},
	urldate = {2017-11-20},
	journal = {Machine Learning Mastery},
	author = {Brownlee, Jason},
	month = apr,
	year = {2016},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/9H9I6C6Y/boosting-and-adaboost-for-machine-learning.html:text/html}
}

@misc{enhancedatascience_machine_2017,
	title = {Machine {Learning} {Explained}: {Bagging}},
	shorttitle = {Machine {Learning} {Explained}},
	url = {http://enhancedatascience.com/2017/06/28/machine-learning-explained-bagging/},
	abstract = {Bagging is a powerful method to improve the performance of simple models and reduce overfitting of more complex models. Let's explore how it works !},
	urldate = {2017-11-20},
	journal = {Enhance Data Science},
	author = {{EnhanceDataScience}},
	month = jun,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/I9UMPZT7/machine-learning-explained-bagging.html:text/html}
}

@article{breiman_bagging_1996,
	title = {Bagging {Predictors}},
	volume = {24},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/A:1018054314350},
	doi = {10.1023/A:1018054314350},
	abstract = {Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy.},
	language = {en},
	number = {2},
	urldate = {2017-11-20},
	journal = {Machine Learning},
	author = {Breiman, Leo},
	month = aug,
	year = {1996},
	pages = {123--140},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/F8MREDJ9/Breiman - 1996 - Bagging Predictors.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/36W5X4IG/A1018054314350.html:text/html}
}

@inproceedings{tu_effective_2009,
	title = {Effective {Diagnosis} of {Heart} {Disease} through {Bagging} {Approach}},
	doi = {10.1109/BMEI.2009.5301650},
	abstract = {The diagnosis of heart disease is important issue, prompting many researchers to work on development of intelligent medical decision support systems to improve the ability of physicians. In this paper, we propose the use of a bagging algorithm to indentify the warning signs of heart disease in patients and also to compare the effectiveness of the bagging algorithm with the decision tree algorithm that has been used by many researchers.},
	booktitle = {2009 2nd {International} {Conference} on {Biomedical} {Engineering} and {Informatics}},
	author = {Tu, M. C. and Shin, D. and Shin, D.},
	month = oct,
	year = {2009},
	keywords = {Machine learning, Bagging, bagging approach, Cardiac disease, cardiology, Coronary arteriosclerosis, Databases, decision support systems, decision tree algorithm comparison, decision trees, Decision trees, heart disease diagnosis, learning (artificial intelligence), medical computing, medical decision support systems, Medical diagnostic imaging, patient diagnosis, Predictive models, Sampling methods, Training data},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/Users/davidfarrugia/Zotero/storage/DHBXPBME/5301650.html:text/html}
}

@article{huang_extreme_2006,
	series = {Neural {Networks}},
	title = {Extreme learning machine: {Theory} and applications},
	volume = {70},
	issn = {0925-2312},
	shorttitle = {Extreme learning machine},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231206000385},
	doi = {10.1016/j.neucom.2005.12.126},
	abstract = {It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: (1) the slow gradient-based learning algorithms are extensively used to train neural networks, and (2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these conventional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for single-hidden layer feedforward neural networks (SLFNs) which randomly chooses hidden nodes and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide good generalization performance at extremely fast learning speed. The experimental results based on a few artificial and real benchmark function approximation and classification problems including very large complex applications show that the new algorithm can produce good generalization performance in most cases and can learn thousands of times faster than conventional popular learning algorithms for feedforward neural networks.11For the preliminary idea of the ELM algorithm, refer to “Extreme Learning Machine: A New Learning Scheme of Feedforward Neural Networks”, Proceedings of International Joint Conference on Neural Networks (IJCNN2004), Budapest, Hungary, 25–29 July, 2004.},
	number = {1},
	urldate = {2017-11-20},
	journal = {Neurocomputing},
	author = {Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
	month = dec,
	year = {2006},
	keywords = {Back-propagation algorithm, Extreme learning machine, Feedforward neural networks, Random node, Real-time learning, Support vector machine},
	pages = {489--501},
	file = {ScienceDirect Snapshot:/Users/davidfarrugia/Zotero/storage/2BC9DHLF/S0925231206000385.html:text/html}
}

@article{yu_bankruptcy_2014,
	title = {Bankruptcy prediction using {Extreme} {Learning} {Machine} and financial expertise},
	volume = {128},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231213010011},
	doi = {10.1016/j.neucom.2013.01.063},
	abstract = {Bankruptcy prediction has been widely studied as a binary classification problem using financial ratios methodologies. In this paper, Leave-One-Out-Incremental Extreme Learning Machine (LOO-IELM) is explored for this task. LOO-IELM operates in an incremental way to avoid inefficient and unnecessary calculations and stops automatically with the neurons of which the number is unknown. Moreover, Combo method and further Ensemble model are investigated based on different LOO-IELM models and the specific financial indicators. These indicators are chosen using different strategies according to the financial expertise. The entire process has shown its good performance with a very fast speed, and also helps to interpret the model and the special ratios.},
	number = {Supplement C},
	urldate = {2017-11-20},
	journal = {Neurocomputing},
	author = {Yu, Qi and Miche, Yoan and Séverin, Eric and Lendasse, Amaury},
	month = mar,
	year = {2014},
	keywords = {Bankruptcy Prediction, Extreme Learning Machine, Incremental Learning, Leave-One-Out},
	pages = {296--302},
	file = {ScienceDirect Snapshot:/Users/davidfarrugia/Zotero/storage/A96KRN8B/S0925231213010011.html:text/html}
}

@article{kuang_tensor-based_2014,
	title = {A {Tensor}-{Based} {Approach} for {Big} {Data} {Representation} and {Dimensionality} {Reduction}},
	volume = {2},
	issn = {2168-6750},
	doi = {10.1109/TETC.2014.2330516},
	abstract = {Variety and veracity are two distinct characteristics of large-scale and heterogeneous data. It has been a great challenge to efficiently represent and process big data with a unified scheme. In this paper, a unified tensor model is proposed to represent the unstructured, semistructured, and structured data. With tensor extension operator, various types of data are represented as subtensors and then are merged to a unified tensor. In order to extract the core tensor which is small but contains valuable information, an incremental high order singular value decomposition (IHOSVD) method is presented. By recursively applying the incremental matrix decomposition algorithm, IHOSVD is able to update the orthogonal bases and compute the new core tensor. Analyzes in terms of time complexity, memory usage, and approximation accuracy of the proposed method are provided in this paper. A case study illustrates that approximate data reconstructed from the core set containing 18\% elements can guarantee 93\% accuracy in general. Theoretical analyzes and experimental results demonstrate that the proposed unified tensor model and IHOSVD method are efficient for big data representation and dimensionality reduction.},
	number = {3},
	journal = {IEEE Transactions on Emerging Topics in Computing},
	author = {Kuang, L. and Hao, F. and Yang, L. T. and Lin, M. and Luo, C. and Min, G.},
	month = sep,
	year = {2014},
	keywords = {approximation accuracy, Approximation methods, approximation theory, Big data, Big Data, Big Data process, Big Data representation, computational complexity, core tensor extraction, Data models, data reconstruction, data representation, data structures, database theory, dimensionality reduction, heterogeneous data, HOSVD, IHOSVD method, incremental high order singular value decomposition method, incremental matrix decomposition algorithm, large-scale data, Large-scale systems, memory usage, orthogonal bases, semistructured data, singular value decomposition, subtensors, Tensile stress, Tensor, tensor extension operator, tensor-based approach, tensors, time complexity, unified tensor model, variety characteristics, veracity characteristics, XML},
	pages = {280--291},
	file = {IEEE Xplore Abstract Record:/Users/davidfarrugia/Zotero/storage/JKEFQ6Q7/6832490.html:text/html;IEEE Xplore Full Text PDF:/Users/davidfarrugia/Zotero/storage/8PEWDQTG/Kuang et al. - 2014 - A Tensor-Based Approach for Big Data Representatio.pdf:application/pdf}
}

@article{tan_tensor-based_2013,
	series = {Euro {Transportation}: selected paper from the {EWGT} {Meeting}, {Padova}, {September} 2009},
	title = {A tensor-based method for missing traffic data completion},
	volume = {28},
	issn = {0968-090X},
	url = {http://www.sciencedirect.com/science/article/pii/S0968090X12001532},
	doi = {10.1016/j.trc.2012.12.007},
	abstract = {Missing and suspicious traffic data are inevitable due to detector and communication malfunctions, which adversely affect the transportation management system (TMS). In this paper, a tensor pattern which is an extension of matrix is introduced into modeling the traffic data for the first time, which can give full play to traffic spatial–temporal information and preserve the multi-way nature of traffic data. To estimate the missing value, a tensor decomposition based Imputation method has been developed. This approach not only inherits the advantages of imputation methods based on matrix pattern for estimating missing points, but also well mines the multi-dimensional inherent correlation of traffic data. Experiments demonstrate that the proposed method achieves a better imputation performance than the state-of-the-art imputation approach even when the missing ratio is up to 90\%. Furthermore, the experimental results show that the proposed method can address the extreme case where the data of one or several days are completely missing, and additionally it can be employed to recover the missing traffic data in adverse weather as well.},
	number = {Supplement C},
	urldate = {2017-11-20},
	journal = {Transportation Research Part C: Emerging Technologies},
	author = {Tan, Huachun and Feng, Guangdong and Feng, Jianshuai and Wang, Wuhong and Zhang, Yu-Jin and Li, Feng},
	month = mar,
	year = {2013},
	keywords = {Missing data, Multiple pattern, Tensor decomposition, Traffic volume},
	pages = {15--27},
	file = {ScienceDirect Snapshot:/Users/davidfarrugia/Zotero/storage/W4AWZVG9/S0968090X12001532.html:text/html}
}

@inproceedings{wang_dynamic_2010,
	title = {Dynamic {Adaboost} ensemble extreme learning machine},
	volume = {3},
	doi = {10.1109/ICACTE.2010.5579726},
	abstract = {This paper proposes a new algorithm: dynamic Adaboost ensemble extreme learning machine, which regards the extreme learning machine as weak learning machine, dynamic Adaboost ensemble algorithm is used to integrate the outputs of weak learning machines, and makes use of fuzzy activation function as activation function of extreme learning machine because of low computational burden and easy implementation in hardware. Proposed algorithm has been successfully applied to problem of function approximation and classification application. Experimental results show that the algorithm increases the training speed greatly when dealing with large dataset and has better generalization performance than extreme learning machine algorithm and Boosting ensemble extreme learning machine with Quasi-Newton algorithm.},
	booktitle = {2010 3rd {International} {Conference} on {Advanced} {Computer} {Theory} and {Engineering}({ICACTE})},
	author = {Wang, Gaitang and Li, Ping},
	month = aug,
	year = {2010},
	keywords = {learning (artificial intelligence), Benchmark testing, Classification algorithms, dynamic Adaboost ensemble, dynamic Adaboost ensemble extreme learning machine, extreme learning machine, function approximation, fuzzy activation function, generalisation (artificial intelligence), generalization performance, Quasi-Newton algorithm, Robots},
	pages = {V3--54--V3--58},
	file = {IEEE Xplore Abstract Record:/Users/davidfarrugia/Zotero/storage/4HDAZ6PJ/5579726.html:text/html}
}

@article{catak_classification_2017,
	title = {Classification with boosting of extreme learning machine over arbitrarily partitioned data},
	volume = {21},
	issn = {1432-7643, 1433-7479},
	url = {https://link.springer.com/article/10.1007/s00500-015-1938-4},
	doi = {10.1007/s00500-015-1938-4},
	abstract = {Machine learning-based computational intelligence methods are widely used to analyze large-scale data sets in this age of big data. Extracting useful predictive modeling from these types of data sets is a challenging problem due to their high complexity. Analyzing large amount of streaming data that can be leveraged to derive business value is another complex problem to solve. With high levels of data availability (i.e., Big Data), automatic classification of them has become an important and complex task. Hence, we explore the power of applying MapReduce-based distributed AdaBoosting of extreme learning machine (ELM) to build a predictive bag of classification models. Accordingly, (1) data set ensembles are created; (2) ELM algorithm is used to build weak learners (classifier functions); and (3) builds a strong learner from a set of weak learners. We applied this training model to the benchmark knowledge discovery and data mining data sets.},
	language = {en},
	number = {9},
	urldate = {2017-11-20},
	journal = {Soft Comput},
	author = {Catak, Ferhat Ozgur},
	month = may,
	year = {2017},
	pages = {2269--2281},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/ANXIH4XW/s00500-015-1938-4.html:text/html}
}

@inproceedings{saabni_ada-boosting_2015,
	title = {Ada-boosting {Extreme} learning machines for handwritten digit and digit strings recognition},
	doi = {10.1109/ICDIPC.2015.7323034},
	abstract = {Automatic handwriting recognition of digit strings, is of academic and commercial interest. Current algorithms are already quite good at learning to recognize handwritten digits, which enables to use them for sorting letters and reading personal checks. Neural networks are a powerful technology for classification of visual inputs arising from documents, and have been extensively used in many fields due to their ability to approximate complex nonlinear mappings directly from the input sample. Different variations of Multi-Layer Neural Networks (MLNN), using back propagation training algorithm, yield a very high recognition rates on handwritten digits benchmarks, but lacks the aspect of speed in training time. Learning time is an important factor while designing any computational intelligent algorithm for classifications, especially when online improving by adapting new samples is needed. Extreme Learning Machine (ELM) has been proposed as an alternative ANN, which significantly reduce the amount of time needed to train a MLNN and has been widely used for many applications. The ELM analytical process of learning reduces the time of learning comparing to back propagation by avoiding the process of iterative learning. In this paper, we present a process which boosts few Extreme learning machines using Ada-boosting in order to improve the recognition rates iteratively. A pre-processing step is used to improve the ability of the ELM, and special weighting process to improve the boosting process. To evaluate the presented approach, we have used the (HDRC 2013) data-set which have bee used at the 2014 competition on handwritten digit string recognition organized in conjunction with ICFHR2014 of Western Arabic digit string recognition with varying length. Very high accurate results in terms of very low error rates while keeping efficient time of online training were achieved by the presented approach, which enables on demand time/precision tradeoff.},
	booktitle = {2015 {Fifth} {International} {Conference} on {Digital} {Information} {Processing} and {Communications} ({ICDIPC})},
	author = {Saabni, R.},
	month = oct,
	year = {2015},
	keywords = {Benchmark testing, Ada-boost, Ada-boosting extreme learning machines, backpropagation, backpropagation training algorithm, Biological neural networks, Boosting, computational intelligent algorithm, document image processing, ELM, Error analysis, Extreme Learning Machines, Handwriting recognition, handwritten character recognition, Handwritten Digit Recognition, handwritten digit string recognition, handwritten documents, HDRC 2013 dataset, image classification, MLNN, multilayer neural networks, multilayer perceptrons, Neural Networks, Neurons, Training, visual input classification, Western Arabic digit string recognition},
	pages = {231--236},
	file = {IEEE Xplore Abstract Record:/Users/davidfarrugia/Zotero/storage/MDNKF366/7323034.html:text/html}
}

@article{charalampous_tensor-based_2014,
	title = {A {Tensor}-{Based} {Deep} {Learning} {Framework}},
	volume = {32},
	doi = {10.1016/j.imavis.2014.08.003},
	abstract = {This paper presents an unsupervised deep learning framework that derives spatio-temporal features for human-robot interaction. The respective models extract high-level features from low-level ones through a hierarchical network, viz. the Hierarchical Temporal Memory (HTM), providing at the same time a solution to the curse of dimensionality in shallow techniques. The presented work incorporates the tensor-based framework within the operation of the nodes and, thus, enhances the feature derivation procedure. This is due to the fact that tensors allow the preservation of the initial data format and their respective correlation and, moreover, attain more compact representations. The computational nodes form spatial and temporal groups by exploiting the multilinear algebra, subsequently express the samples according to those groups in terms of proximity. This generic framework may be applied in a diverse of visual data, whilst it has been examined on sequences of color and depth images, exhibiting remarkable performance.},
	journal = {Image and Vision Computing},
	author = {Charalampous, Konstantinos and Gasteratos, Antonios},
	month = nov,
	year = {2014}
}

@article{tucker_mathematical_1966,
	title = {Some mathematical notes on three-mode factor analysis},
	volume = {31},
	issn = {0033-3123, 1860-0980},
	url = {https://link.springer.com/article/10.1007/BF02289464},
	doi = {10.1007/BF02289464},
	abstract = {The model for three-mode factor analysis is discussed in terms of newer applications of mathematical processes including a type of matrix process termed the Kronecker product and the definition of combination variables. Three methods of analysis to a type of extension of principal components analysis are discussed. Methods II and III are applicable to analysis of data collected for a large sample of individuals. An extension of the model is described in which allowance is made for unique variance for each combination variable when the data are collected for a large sample of individuals.},
	language = {en},
	number = {3},
	urldate = {2017-11-20},
	journal = {Psychometrika},
	author = {Tucker, Ledyard R.},
	month = sep,
	year = {1966},
	pages = {279--311},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/SLP2TNSN/BF02289464.html:text/html}
}

@inproceedings{li_tensor-based_2015,
	title = {Tensor-{Based} {Learning} for {Predicting} {Stock} {Movements}},
	copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys’ fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author’s personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author’s employer, and then only on the author’s or the employer’s own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author’s or the employer’s creation (including tables of contents with links to other papers) without AAAI’s written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
	url = {https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/9409},
	abstract = {Stock  movements are essentially driven by new information. Market data, financial news, and social sentiment are believed to have impacts on stock markets. To study the correlation between information and stock movements, previous works typically concatenate the features of different information sources into one super feature vector. However, such concatenated vector approaches  treat each information source separately and ignore their interactions. In this article, we model the multi-faceted investors’ information and their intrinsic links with tensors. To identify the  nonlinear patterns between stock movements and new information, we propose a supervised tensor regression learning approach to investigate the joint impact of different information sources on stock markets. Experiments on CSI 100 stocks in the year 2011 show that our  approach outperforms the state-of-the-art  trading strategies.},
	language = {en},
	urldate = {2017-11-21},
	booktitle = {Twenty-{Ninth} {AAAI} {Conference} on {Artificial} {Intelligence}},
	author = {Li, Qing and Jiang, LiLing and Li, Ping and Chen, Hsinchun},
	month = feb,
	year = {2015},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/MSDYJJU6/Li et al. - 2015 - Tensor-Based Learning for Predicting Stock Movemen.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/DY8S4544/9409.html:text/html}
}

@article{huang_real-time_2006,
	title = {Real-time learning capability of neural networks},
	volume = {17},
	issn = {1045-9227},
	doi = {10.1109/TNN.2006.875974},
	abstract = {In some practical applications of neural networks, fast response to external events within an extremely short time is highly demanded and expected. However, the extensively used gradient-descent-based learning algorithms obviously cannot satisfy the real-time learning needs in many applications, especially for large-scale applications and/or when higher generalization performance is required. Based on Huang's constructive network model, this paper proposes a simple learning algorithm capable of real-time learning which can automatically select appropriate values of neural quantizers and analytically determine the parameters (weights and bias) of the network at one time only. The performance of the proposed algorithm has been systematically investigated on a large batch of benchmark real-world regression and classification problems. The experimental results demonstrate that our algorithm can not only produce good generalization performance but also have real-time learning and prediction capability. Thus, it may provide an alternative approach for the practical applications of neural networks where real-time learning and prediction implementation is required.},
	language = {eng},
	number = {4},
	journal = {IEEE Trans Neural Netw},
	author = {Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
	month = jul,
	year = {2006},
	pmid = {16856651},
	keywords = {Computer Systems, Learning, Neural Networks (Computer)},
	pages = {863--878}
}

@article{huang_extreme_2011,
	title = {Extreme learning machines: a survey},
	volume = {2},
	issn = {1868-8071, 1868-808X},
	shorttitle = {Extreme learning machines},
	url = {https://link.springer.com/article/10.1007/s13042-011-0019-y},
	doi = {10.1007/s13042-011-0019-y},
	abstract = {Computational intelligence techniques have been used in wide applications. Out of numerous computational intelligence techniques, neural networks and support vector machines (SVMs) have been playing the dominant roles. However, it is known that both neural networks and SVMs face some challenging issues such as: (1) slow learning speed, (2) trivial human intervene, and/or (3) poor computational scalability. Extreme learning machine (ELM) as emergent technology which overcomes some challenges faced by other techniques has recently attracted the attention from more and more researchers. ELM works for generalized single-hidden layer feedforward networks (SLFNs). The essence of ELM is that the hidden layer of SLFNs need not be tuned. Compared with those traditional computational intelligence techniques, ELM provides better generalization performance at a much faster learning speed and with least human intervene. This paper gives a survey on ELM and its variants, especially on (1) batch learning mode of ELM, (2) fully complex ELM, (3) online sequential ELM, (4) incremental ELM, and (5) ensemble of ELM.},
	language = {en},
	number = {2},
	urldate = {2017-11-21},
	journal = {Int. J. Mach. Learn. \& Cyber.},
	author = {Huang, Guang-Bin and Wang, Dian Hui and Lan, Yuan},
	month = jun,
	year = {2011},
	pages = {107--122},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/BUKARNZJ/s13042-011-0019-y.html:text/html}
}

@inproceedings{deng_regularized_2009,
	title = {Regularized {Extreme} {Learning} {Machine}},
	doi = {10.1109/CIDM.2009.4938676},
	abstract = {Extreme learning machine proposed by Huang G-B has attracted many attentions for its extremely fast training speed and good generalization performance. But it still can be considered as empirical risk minimization theme and tends to generate over-fitting model. Additionally, since ELM doesn't considering heteroskedasticity in real applications, its performance will be affected seriously when outliers exist in the dataset. In order to address these drawbacks, we propose a novel algorithm called regularized extreme learning machine based on structural risk minimization principle and weighted least square. The generalization performance of the proposed algorithm was improved significantly in most cases without increasing training time.},
	booktitle = {2009 {IEEE} {Symposium} on {Computational} {Intelligence} and {Data} {Mining}},
	author = {Deng, W. and Zheng, Q. and Chen, L.},
	month = mar,
	year = {2009},
	keywords = {Machine learning, learning (artificial intelligence), Feedforward neural networks, Neurons, Computer science, feedforward neural nets, Joining processes, Least Square, Least squares methods, Mathematical model, minimisation, Multi-layer neural network, Neural networks, over-fitting model, regularized extreme learning machine, Risk management, risk minimization, structural risk minimization, weighted least square},
	pages = {389--395},
	file = {IEEE Xplore Abstract Record:/Users/davidfarrugia/Zotero/storage/U4575VVF/4938676.html:text/html}
}

@article{huang_convex_2007,
	series = {Neural {Network} {Applications} in {Electrical} {Engineering}},
	title = {Convex incremental extreme learning machine},
	volume = {70},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231207000677},
	doi = {10.1016/j.neucom.2007.02.009},
	abstract = {Unlike the conventional neural network theories and implementations, Huang et al. [Universal approximation using incremental constructive feedforward networks with random hidden nodes, IEEE Transactions on Neural Networks 17(4) (2006) 879–892] have recently proposed a new theory to show that single-hidden-layer feedforward networks (SLFNs) with randomly generated additive or radial basis function (RBF) hidden nodes (according to any continuous sampling distribution) can work as universal approximators and the resulting incremental extreme learning machine (I-ELM) outperforms many popular learning algorithms. I-ELM randomly generates the hidden nodes and analytically calculates the output weights of SLFNs, however, I-ELM does not recalculate the output weights of all the existing nodes when a new node is added. This paper shows that while retaining the same simplicity, the convergence rate of I-ELM can be further improved by recalculating the output weights of the existing nodes based on a convex optimization method when a new hidden node is randomly added. Furthermore, we show that given a type of piecewise continuous computational hidden nodes (possibly not neural alike nodes), if SLFNs fn(x)=∑i=1nβiG(x,ai,bi) can work as universal approximators with adjustable hidden node parameters, from a function approximation point of view the hidden node parameters of such “generalized” SLFNs (including sigmoid networks, RBF networks, trigonometric networks, threshold networks, fuzzy inference systems, fully complex neural networks, high-order networks, ridge polynomial networks, wavelet networks, etc.) can actually be randomly generated according to any continuous sampling distribution. In theory, the parameters of these SLFNs can be analytically determined by ELM instead of being tuned.},
	number = {16},
	urldate = {2017-11-21},
	journal = {Neurocomputing},
	author = {Huang, Guang-Bin and Chen, Lei},
	month = oct,
	year = {2007},
	keywords = {Convergence rate, Convex optimization, Generalized feedforward networks, Incremental extreme learning machine, Random hidden nodes},
	pages = {3056--3062},
	file = {ScienceDirect Snapshot:/Users/davidfarrugia/Zotero/storage/BAGKLP33/S0925231207000677.html:text/html}
}

@techreport{fiedler_gambling_2011,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The {Gambling} {Habits} of {Online} {Poker} {Players}},
	url = {https://papers.ssrn.com/abstract=1908161},
	abstract = {Online poker is a data goldmine. Recording actual gambling behavior gives rise to a host of research opportunities. Still, investigations using such data are rare with the excep-tion of nine pioneering studies by Harvard Medical School which are reviewed here. This paper fills part of the vacuum by analyzing the gambling habits of a sample of 2,127,887 poker playing identities at Pokerstars over a period of six months. A couple of playing variables are operationalized and were analyzed on their own as well as connected with each other in form of the playing volume (\$ rake a player has paid in a time frame).The main findings confirm the results of the Harvard studies: most online poker players only play a few times and for very low stakes. The median player played 7 sessions and 4.87 hours over 6 months. Multitabling was observed only rarely (median 1.05) and most players pay very low fees per hour (median US\$0.87 per hour per table). The playing volume is very low, too, with more than 50\% of all players paying less than US\$4.86 to the operators over 6 months. An analysis of the relationship between the playing habits shows that they reinforce each other with the exception of the playing frequency which moderates gambling involvement. The average values of the playing habits are considerably higher due to a small group of intense players: the 99\% percentile player has a playing volume that is 552 times higher than that of the median player (US\$2,685), and 1\% of the players account for 60\% of playing volume (10\% for even 91\%). This group is analyzed more thoroughly, and a discussion shows that the first impulse to peg intense players as (probable) pathological gamblers is wrong. Rather, future research is needed to distinguish problem gamblers from professional players.},
	number = {ID 1908161},
	urldate = {2017-11-22},
	institution = {Social Science Research Network},
	author = {Fiedler, Ingo},
	month = sep,
	year = {2011},
	keywords = {behavior, gambling, habits, online, poker},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/5WFY27UT/papers.html:text/html}
}

@misc{gupta_decision_2017,
	title = {Decision {Trees} in {Machine} {Learning}},
	url = {https://towardsdatascience.com/decision-trees-in-machine-learning-641b9c4e8052},
	abstract = {A tree has many analogies in real life, and turns out that it has influenced a wide area of machine learning, covering both classification…},
	urldate = {2018-04-08},
	journal = {Towards Data Science},
	author = {Gupta, Prashant},
	month = may,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/4QNKF4IY/decision-trees-in-machine-learning-641b9c4e8052.html:text/html}
}

@article{friedl_decision_1997,
	title = {Decision {Tree} {Classification} of {Land} {Cover} from {Remotely} {Sensed} {Data}},
	volume = {61},
	doi = {10.1016/S0034-4257(97)00049-7},
	abstract = {Decision tree classification algorithms have significant potential for land cover mapping problems and have not been tested in detail by the remote sensing community relative to more conventional pattern recognition techniques such as maximum likelihood classification. In this paper, we present several types of decision tree classification algorithms arid evaluate them on three different remote sensing data sets. The decision tree classification algorithms tested include an univariate decision tree, a multivariate decision tree, and a hybrid decision tree capable of including several different types of classification algorithms within a single decision tree structure. Classification accuracies produced by each of these decision tree algorithms are compared with both maximum likelihood and linear discriminant function classifiers. Results from this analysis show that the decision tree algorithms consistently outperform the maximum likelihood and linear discriminant function classifiers in regard to classf — cation accuracy. In particular, the hybrid tree consistently produced the highest classification accuracies for the data sets tested. More generally, the results from this work show that decision trees have several advantages for remote sensing applications by virtue of their relatively simple, explicit, and intuitive classification structure. Further, decision tree algorithms are strictly nonparametric and, therefore, make no assumptions regarding the distribution of input data, and are flexible and robust with respect to nonlinear and noisy relations among input features and class labels.},
	journal = {Remote Sensing of Environment},
	author = {Friedl, M.A. and Brodley, Carla},
	month = sep,
	year = {1997},
	pages = {399--409}
}

@misc{emily_fox_boosting_nodate,
	title = {The boosting question - {Boosting}},
	url = {https://www.coursera.org/learn/ml-classification/lecture/3ywWA/the-boosting-question},
	abstract = {Video created by University of Washington for the course "Machine Learning: Classification". One of the most exciting theoretical questions that have been asked about machine learning is whether simple classifiers can be combined into a highly ...},
	language = {en},
	urldate = {2018-04-08},
	journal = {Coursera},
	author = {{Emily Fox} and Guestrin, Carlos},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/WPAC5USR/the-boosting-question.html:text/html}
}

@article{geurts_extremely_2006,
	title = {Extremely randomized trees},
	volume = {63},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1007/s10994-006-6226-1},
	doi = {10.1007/s10994-006-6226-1},
	abstract = {This paper proposes a new tree-based ensemble method for supervised classification and regression problems. It essentially consists of randomizing strongly both attribute and cut-point choice while splitting a tree node. In the extreme case, it builds totally randomized trees whose structures are independent of the output values of the learning sample. The strength of the randomization can be tuned to problem specifics by the appropriate choice of a parameter. We evaluate the robustness of the default choice of this parameter, and we also provide insight on how to adjust it in particular situations. Besides accuracy, the main strength of the resulting algorithm is computational efficiency. A bias/variance analysis of the Extra-Trees algorithm is also provided as well as a geometrical and a kernel characterization of the models induced.},
	language = {en},
	number = {1},
	urldate = {2018-04-08},
	journal = {Mach Learn},
	author = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
	month = apr,
	year = {2006},
	pages = {3--42},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/N5ZBFIUI/Geurts et al. - 2006 - Extremely randomized trees.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/HIB7H97P/s10994-006-6226-1.html:text/html}
}

@inproceedings{chen_xgboost:_2016,
	address = {New York, NY, USA},
	series = {{KDD} '16},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	isbn = {978-1-4503-4232-2},
	shorttitle = {{XGBoost}},
	url = {http://doi.acm.org/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly effective and widely used machine learning method. In this paper, we describe a scalable end-to-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	urldate = {2018-04-09},
	booktitle = {Proceedings of the 22Nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Tianqi and Guestrin, Carlos},
	year = {2016},
	keywords = {large-scale, learning, machine},
	pages = {785--794},
	file = {ACM Full Text PDF:/Users/davidfarrugia/Zotero/storage/W7TUMNH8/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf}
}

@misc{analytics_vidhya_which_2017,
	title = {Which algorithm takes the crown: {Light} {GBM} vs {XGBOOST}?},
	shorttitle = {Which algorithm takes the crown},
	url = {https://www.analyticsvidhya.com/blog/2017/06/which-algorithm-takes-the-crown-light-gbm-vs-xgboost/},
	abstract = {A comparison between LightGBM and XGBoost algorithms in machine learning. XGBoost works on lead based splitting of decision tree \& is faster, parallel},
	urldate = {2018-04-09},
	journal = {Analytics Vidhya},
	author = {{Analytics Vidhya}},
	month = jun,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/B338Z7R9/which-algorithm-takes-the-crown-light-gbm-vs-xgboost.html:text/html}
}

@article{torlay_machine_2017,
	title = {Machine learning-{XGBoost} analysis of language networks to classify patients with epilepsy},
	volume = {4},
	issn = {2198-4018},
	doi = {10.1007/s40708-017-0065-7},
	abstract = {Our goal was to apply a statistical approach to allow the identification of atypical language patterns and to differentiate patients with epilepsy from healthy subjects, based on their cerebral activity, as assessed by functional MRI (fMRI). Patients with focal epilepsy show reorganization or plasticity of brain networks involved in cognitive functions, inducing 'atypical' (compared to 'typical' in healthy people) brain profiles. Moreover, some of these patients suffer from drug-resistant epilepsy, and they undergo surgery to stop seizures. The neurosurgeon should only remove the zone generating seizures and must preserve cognitive functions to avoid deficits. To preserve functions, one should know how they are represented in the patient's brain, which is in general different from that of healthy subjects. For this purpose, in the pre-surgical stage, robust and efficient methods are required to identify atypical from typical representations. Given the frequent location of regions generating seizures in the vicinity of language networks, one important function to be considered is language. The risk of language impairment after surgery is determined pre-surgically by mapping language networks. In clinical settings, cognitive mapping is classically performed with fMRI. The fMRI analyses allowing the identification of atypical patterns of language networks in patients are not sufficiently robust and require additional statistic approaches. In this study, we report the use of a statistical nonlinear machine learning classification, the Extreme Gradient Boosting (XGBoost) algorithm, to identify atypical patterns and classify 55 participants as healthy subjects or patients with epilepsy. XGBoost analyses were based on neurophysiological features in five language regions (three frontal and two temporal) in both hemispheres and activated with fMRI for a phonological (PHONO) and a semantic (SEM) language task. These features were combined into 135 cognitively plausible subsets and further submitted to selection and binary classification. Classification performance was scored with the Area Under the receiver operating characteristic curve (AUC). Our results showed that the subset SEM\_LH BA\_47-21 (left fronto-temporal activation induced by the SEM task) provided the best discrimination between the two groups (AUC of 91±5\%). The results are discussed in the framework of the current debates of language reorganization in focal epilepsy.},
	language = {eng},
	number = {3},
	journal = {Brain Inform},
	author = {Torlay, L. and Perrone-Bertolotti, M. and Thomas, E. and Baciu, M.},
	month = sep,
	year = {2017},
	pmid = {28434153},
	pmcid = {PMC5563301},
	keywords = {Machine learning, Atypical, Epilepsy, Extreme Gradient Boosting, Language, ML, XGBoost},
	pages = {159--169}
}

@article{tamayo_machine_2016,
	title = {A {Machine} {Learns} to {Predict} the {Stability} of {Tightly} {Packed} {Planetary} {Systems}},
	volume = {832},
	issn = {2041-8205},
	url = {http://stacks.iop.org/2041-8205/832/i=2/a=L22},
	doi = {10.3847/2041-8205/832/2/L22},
	abstract = {The requirement that planetary systems be dynamically stable is often used to vet new discoveries or set limits on unconstrained masses or orbital elements. This is typically carried out via computationally expensive N -body simulations. We show that characterizing the complicated and multi-dimensional stability boundary of tightly packed systems is amenable to machine-learning methods. We find that training an XGBoost machine-learning algorithm on physically motivated features yields an accurate classifier of stability in packed systems. On the stability timescale investigated (10 7 orbits), it is three orders of magnitude faster than direct N -body simulations. Optimized machine-learning classifiers for dynamical stability may thus prove useful across the discipline, e.g., to characterize the exoplanet sample discovered by the upcoming Transiting Exoplanet Survey Satellite . This proof of concept motivates investing computational resources to train algorithms capable of predicting stability over longer timescales and over broader regions of phase space.},
	language = {en},
	number = {2},
	urldate = {2018-04-09},
	journal = {ApJL},
	author = {Tamayo, Daniel and Silburt, Ari and Valencia, Diana and Menou, Kristen and Ali-Dib, Mohamad and Petrovich, Cristobal and Huang, Chelsea X. and Rein, Hanno and Laerhoven, Christa van and Paradise, Adiv and Obertas, Alysa and Murray, Norman},
	year = {2016},
	pages = {L22},
	file = {IOP Full Text PDF:/Users/davidfarrugia/Zotero/storage/DBU4AKX7/Tamayo et al. - 2016 - A Machine Learns to Predict the Stability of Tight.pdf:application/pdf}
}

@incollection{ke_lightgbm:_2017,
	title = {{LightGBM}: {A} {Highly} {Efficient} {Gradient} {Boosting} {Decision} {Tree}},
	shorttitle = {{LightGBM}},
	url = {http://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf},
	urldate = {2018-04-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 30},
	publisher = {Curran Associates, Inc.},
	author = {Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
	pages = {3146--3154},
	file = {NIPS Full Text PDF:/Users/davidfarrugia/Zotero/storage/3CLQBED5/Ke et al. - 2017 - LightGBM A Highly Efficient Gradient Boosting Dec.pdf:application/pdf;NIPS Snapshort:/Users/davidfarrugia/Zotero/storage/NPPT92T6/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.html:text/html}
}

@misc{kaggle_allstate_nodate,
	title = {Allstate {Claim} {Prediction} {Challenge}},
	url = {https://www.kaggle.com/c/ClaimPredictionChallenge},
	abstract = {A key part of insurance is charging each customer the appropriate price for the risk they represent.},
	urldate = {2018-04-10},
	author = {{Kaggle}},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/2IQIBL25/ClaimPredictionChallenge.html:text/html}
}

@inproceedings{wang_lightgbm:_2017,
	address = {New York, NY, USA},
	series = {{ICCBB} 2017},
	title = {{LightGBM}: {An} {Effective} {miRNA} {Classification} {Method} in {Breast} {Cancer} {Patients}},
	isbn = {978-1-4503-5322-9},
	shorttitle = {{LightGBM}},
	url = {http://doi.acm.org/10.1145/3155077.3155079},
	doi = {10.1145/3155077.3155079},
	abstract = {miRNAs are small noncoding RNA molecules, mainly responsible for post-transcriptional control of gene expressions. Machine learning is becoming more and more widely used in breast tumor classification and diagnosis. In this paper, we compared the performance of different machine learning methods, such as Random Forest (RF), eXtreme Gradient Boosting(XGBoost) and Light Gradient Boosting Machine(LightGBM), for miRNAs identification in breast cancer patients. The performance comparison of each algorithm was evaluated based on the accuracy and logistic loss and where LightGBM was found better performing in several aspects. hsa-mir-139 was found as an important target for the breast cancer classification. As a powerful tool, LightGBM can be used to identify and classify miRNA target in breast cancer.},
	urldate = {2018-04-10},
	booktitle = {Proceedings of the 2017 {International} {Conference} on {Computational} {Biology} and {Bioinformatics}},
	publisher = {ACM},
	author = {Wang, Dehua and Zhang, Yang and Zhao, Yi},
	year = {2017},
	keywords = {Classification, LightGBM, miRNA, RF, XGBoost},
	pages = {7--11}
}

@misc{noauthor_lightgbm:_2018,
	title = {{LightGBM}: {A} fast, distributed, high performance gradient boosting ({GBDT}, {GBRT}, {GBM} or {MART}) framework based on decision tree algorithms, used for ranking, classification and many other machine lea..},
	shorttitle = {{LightGBM}},
	url = {https://github.com/Microsoft/LightGBM},
	urldate = {2018-04-10},
	publisher = {Microsoft},
	month = apr,
	year = {2018},
	note = {original-date: 2016-08-05T05:45:50Z},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/PG7U5H88/Experiments.html:text/html}
}

@misc{noel_bambrick_support_2016,
	title = {Support {Vector} {Machines}: {A} {Simple} {Explanation}},
	url = {https://www.kdnuggets.com/2016/07/support-vector-machines-simple-explanation.html},
	abstract = {In this post, we are going to introduce you to the Support Vector Machine (SVM) machine learning algorithm. We will follow a similar process to our recent post Naive Bayes for Dummies; A Simple Explanation by keeping it short and not overly-technical. The aim is to give those of you who are new to machine learning a basic understanding of the key concepts of this algorithm.},
	language = {English},
	journal = {KDnuggets},
	author = {{Noel Bambrick}},
	month = jul,
	year = {2016}
}

@misc{noauthor_understanding_2017,
	title = {Understanding {Support} {Vector} {Machine} algorithm from examples (along with code)},
	url = {https://www.analyticsvidhya.com/blog/2017/09/understaing-support-vector-machine-example-code/},
	abstract = {This article explains support vector machine, a machine learning algorithm and its uses in classification and regression. Its a supervised learning algorithm},
	urldate = {2018-04-11},
	journal = {Analytics Vidhya},
	month = sep,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/ZGEBHHFF/understaing-support-vector-machine-example-code.html:text/html}
}

@article{guyon_gene_2002,
	title = {Gene {Selection} for {Cancer} {Classification} using {Support} {Vector} {Machines}},
	volume = {46},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1023/A:1012487302797},
	doi = {10.1023/A:1012487302797},
	abstract = {DNA micro-arrays now permit scientists to screen thousands of genes simultaneously and determine whether those genes are active, hyperactive or silent in normal or cancerous tissue. Because these new micro-array devices generate bewildering amounts of raw data, new analytical methods must be developed to sort out whether cancer tissues have distinctive signatures of gene expression over normal tissues or other types of cancer tissues.In this paper, we address the problem of selection of a small subset of genes from broad patterns of gene expression data, recorded on DNA micro-arrays. Using available training examples from cancer and normal patients, we build a classifier suitable for genetic diagnosis, as well as drug discovery. Previous attempts to address this problem select genes with correlation techniques. We propose a new method of gene selection utilizing Support Vector Machine methods based on Recursive Feature Elimination (RFE). We demonstrate experimentally that the genes selected by our techniques yield better classification performance and are biologically relevant to cancer.In contrast with the baseline method, our method eliminates gene redundancy automatically and yields better and more compact gene subsets. In patients with leukemia our method discovered 2 genes that yield zero leave-one-out error, while 64 genes are necessary for the baseline method to get the best result (one leave-one-out error). In the colon cancer database, using only 4 genes our method is 98\% accurate, while the baseline method is only 86\% accurate.},
	language = {en},
	number = {1-3},
	urldate = {2018-04-11},
	journal = {Machine Learning},
	author = {Guyon, Isabelle and Weston, Jason and Barnhill, Stephen and Vapnik, Vladimir},
	month = jan,
	year = {2002},
	pages = {389--422},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/N3YGLWTU/Guyon et al. - 2002 - Gene Selection for Cancer Classification using Sup.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/H2MNS7K5/A1012487302797.html:text/html}
}

@book{fradkin_dimacs_nodate,
	title = {{DIMACS} {Series} in {Discrete} {Mathematics} and {Theoretical} {Computer} {Science} {Support} {Vector} {Machines} for {Classification}},
	abstract = {In many real-life situations we want to be able to assign an object to one of several categories based on some of its characteristics. For example, based on the results of several medical tests we want to be able to say whether a patient has a particular disease or should be recommended a specific treatment.},
	author = {Fradkin, Dmitriy and Muchnik, Ilya},
	file = {Citeseer - Full Text PDF:/Users/davidfarrugia/Zotero/storage/C9VKP736/Fradkin and Muchnik - DIMACS Series in Discrete Mathematics and Theoreti.pdf:application/pdf;Citeseer - Snapshot:/Users/davidfarrugia/Zotero/storage/ICYFKT4F/summary.html:text/html}
}

@article{suykens_least_1999,
	title = {Least {Squares} {Support} {Vector} {Machine} {Classifiers}},
	volume = {9},
	issn = {1370-4621, 1573-773X},
	url = {https://link.springer.com/article/10.1023/A:1018628609742},
	doi = {10.1023/A:1018628609742},
	abstract = {In this letter we discuss a least squares version for support vector machine (SVM) classifiers. Due to equality type constraints in the formulation, the solution follows from solving a set of linear equations, instead of quadratic programming for classical SVM's. The approach is illustrated on a two-spiral benchmark classification problem.},
	language = {en},
	number = {3},
	urldate = {2018-04-11},
	journal = {Neural Processing Letters},
	author = {Suykens, J. a. K. and Vandewalle, J.},
	month = jun,
	year = {1999},
	pages = {293--300}
}

@article{huang_assessment_2002,
	title = {An assessment of support vector machines for land cover classification},
	volume = {23},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431160110040323},
	doi = {10.1080/01431160110040323},
	abstract = {The support vector machine (SVM) is a group of theoretically superior machine learning algorithms. It was found competitive with the best available machine learning algorithms in classifying high-dimensional data sets. This paper gives an introduction to the theoretical development of the SVM and an experimental evaluation of its accuracy, stability and training speed in deriving land cover classifications from satellite images. The SVM was compared to three other popular classifiers, including the maximum likelihood classifier (MLC), neural network classifiers (NNC) and decision tree classifiers (DTC). The impacts of kernel configuration on the performance of the SVM and of the selection of training data and input variables on the four classifiers were also evaluated in this experiment.},
	number = {4},
	urldate = {2018-04-11},
	journal = {International Journal of Remote Sensing},
	author = {Huang, C. and Davis, L. S. and Townshend, J. R. G.},
	month = jan,
	year = {2002},
	pages = {725--749},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/KJFG4TTG/01431160110040323.html:text/html}
}

@article{pal_support_2005,
	title = {Support vector machines for classification in remote sensing},
	volume = {26},
	issn = {0143-1161},
	url = {https://doi.org/10.1080/01431160512331314083},
	doi = {10.1080/01431160512331314083},
	abstract = {Support vector machines (SVM) represent a promising development in machine learning research that is not widely used within the remote sensing community. This paper reports the results of two experiments in which multi‐class SVMs are compared with maximum likelihood (ML) and artificial neural network (ANN) methods in terms of classification accuracy. The two land cover classification experiments use multispectral (Landsat‐7 ETM+) and hyperspectral (DAIS) data, respectively, for test areas in eastern England and central Spain. Our results show that the SVM achieves a higher level of classification accuracy than either the ML or the ANN classifier, and that the SVM can be used with small training datasets and high‐dimensional data.},
	number = {5},
	urldate = {2018-04-11},
	journal = {International Journal of Remote Sensing},
	author = {Pal, M. and Mather, P. M.},
	month = mar,
	year = {2005},
	pages = {1007--1011},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/NZMVCWCT/01431160512331314083.html:text/html}
}

@article{burges_tutorial_1998,
	title = {A {Tutorial} on {Support} {Vector} {Machines} for {Pattern} {Recognition}},
	volume = {2},
	issn = {1384-5810, 1573-756X},
	url = {https://link.springer.com/article/10.1023/A:1009715923555},
	doi = {10.1023/A:1009715923555},
	abstract = {The tutorial starts with an overview of the concepts of VC dimension and structural risk minimization. We then describe linear Support Vector Machines (SVMs) for separable and non-separable data, working through a non-trivial example in detail. We describe a mechanical analogy, and discuss when SVM solutions are unique and when they are global. We describe how support vector training can be practically implemented, and discuss in detail the kernel mapping technique which is used to construct SVM solutions which are nonlinear in the data. We show how Support Vector machines can have very large (even infinite) VC dimension by computing the VC dimension for homogeneous polynomial and Gaussian radial basis function kernels. While very high VC dimension would normally bode ill for generalization performance, and while at present there exists no theory which shows that good generalization performance is guaranteed for SVMs, there are several arguments which support the observed high accuracy of SVMs, which we review. Results of some experiments which were inspired by these arguments are also presented. We give numerous examples and proofs of most of the key theorems. There is new material, and I hope that the reader will find that even old material is cast in a fresh light.},
	language = {en},
	number = {2},
	urldate = {2018-04-11},
	journal = {Data Mining and Knowledge Discovery},
	author = {Burges, Christopher J. C.},
	month = jun,
	year = {1998},
	pages = {121--167},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/HA8HZGYR/A1009715923555.html:text/html}
}

@article{mohandes_support_2004,
	title = {Support vector machines for wind speed prediction},
	volume = {29},
	doi = {10.1016/j.renene.2003.11.009},
	abstract = {This paper introduces support vector machines (SVM), the latest neural network algor-ithm, to wind speed prediction and compares their performance with the multilayer percep-tron (MLP) neural networks. Mean daily wind speed data from Madina city, Saudi Arabia, is used for building and testing both models. Results indicate that SVM compare favorably with the MLP model based on the root mean square errors between the actual and the pre-dicted data. These results are confirmed for a system with order 1 to system with order 11.},
	journal = {Renewable Energy},
	author = {Mohandes, Mohamed and O Halawani, T and Rehman, Shafiqur and Hussain, Ahmed},
	month = may,
	year = {2004},
	pages = {939--947},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/WRCDA4E9/Mohandes et al. - 2004 - Support vector machines for wind speed prediction.pdf:application/pdf}
}

@misc{noauthor_introductory_2018,
	title = {An {Introductory} {Guide} to {Regularized} {Greedy} {Forests} ({RGF}) with a case study in {Python}},
	url = {https://www.analyticsvidhya.com/blog/2018/02/introductory-guide-regularized-greedy-forests-rgf-python/},
	abstract = {Introduction As a data scientist participating in multiple machine learning competition, I am always on the lookout for “not-yet-popular” algorithms. The way I define ...},
	urldate = {2018-04-11},
	journal = {Analytics Vidhya},
	month = feb,
	year = {2018},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/35I49VYG/introductory-guide-regularized-greedy-forests-rgf-python.html:text/html}
}

@misc{noauthor_higgs_nodate,
	title = {Higgs {Boson} {Machine} {Learning} {Challenge}},
	url = {https://www.kaggle.com/c/higgs-boson},
	abstract = {Use the ATLAS experiment to identify the Higgs boson},
	urldate = {2018-04-11}
}

@article{johnson_learning_2014,
	title = {Learning {Nonlinear} {Functions} {Using} {Regularized} {Greedy} {Forest}},
	volume = {36},
	issn = {1939-3539},
	doi = {10.1109/TPAMI.2013.159},
	abstract = {We consider the problem of learning a forest of nonlinear decision rules with general loss functions. The standard methods employ boosted decision trees such as Adaboost for exponential loss and Friedman's gradient boosting for general loss. In contrast to these traditional boosting algorithms that treat a tree learner as a black box, the method we propose directly learns decision forests via fully-corrective regularized greedy search using the underlying forest structure. Our method achieves higher accuracy and smaller models than gradient boosting on many of the datasets we have tested on.},
	language = {eng},
	number = {5},
	journal = {IEEE Trans Pattern Anal Mach Intell},
	author = {Johnson, Rie and Tong Zhang},
	month = may,
	year = {2014},
	pmid = {26353228},
	pages = {942--954}
}

@misc{noauthor_k-nearest_2017,
	title = {K-{Nearest} {Neighbours}},
	url = {https://www.geeksforgeeks.org/k-nearest-neighbours/},
	abstract = {K-Nearest Neighbours is one of the most basic yet essential classification algorithms in Machine Learning. It belongs to the supervised learning domain},
	language = {en-US},
	urldate = {2018-04-12},
	journal = {GeeksforGeeks},
	month = apr,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/M24EI97S/k-nearest-neighbours.html:text/html}
}

@article{yao_regression-based_2006,
	title = {A {Regression}-based {K} nearest neighbor algorithm for gene function prediction from heterogeneous data},
	volume = {7},
	issn = {1471-2105},
	url = {https://doi.org/10.1186/1471-2105-7-S1-S11},
	doi = {10.1186/1471-2105-7-S1-S11},
	abstract = {As a variety of functional genomic and proteomic techniques become available, there is an increasing need for functional analysis methodologies that integrate heterogeneous data sources.},
	number = {1},
	urldate = {2018-04-12},
	journal = {BMC Bioinformatics},
	author = {Yao, Zizhen and Ruzzo, Walter L.},
	month = mar,
	year = {2006},
	keywords = {Gene Pair, Local Regression, Receiver Operating Characteristic, Receiver Operating Characteristic Curve, Support Vector Machine},
	pages = {S11},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/6JVHSTAX/Yao and Ruzzo - 2006 - A Regression-based K nearest neighbor algorithm fo.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/PLITEL86/1471-2105-7-S1-S11.html:text/html}
}

@incollection{larose_knearest_2014,
	edition = {Second Edition},
	title = {k‐{Nearest} {Neighbor} {Algorithm}},
	isbn = {978-1-118-87405-9},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781118874059},
	abstract = {The most common data mining task is that of classification tasks that may be found in nearly every field of endeavor: banking, education, medicine, law and homeland security. The author investigates k‐nearest neighbor algorithm, which is most often used for classification task, although it can also be used for estimation and prediction. Data analysts define distance metrics to measure similarity of records. Combination function combines these similar records to provide a classification decision for the new record. For instance‐based learning methods such as the k‐nearest neighbor algorithm, it is vitally important to have access to a rich database full of as many different combinations of attribute values as possible. The chapter discusses application of the k‐nearest neighbor algorithm using IBM/SPSS modeler, and defines the term stretching the axes.},
	language = {English},
	booktitle = {Discovering {Knowledge} in {Data}: {An} {Introduction} to {Data} {Mining}.},
	publisher = {John Wiley \& Sons, Inc.},
	author = {Larose, Daniel T. and Larose, Chantal D.},
	month = jul,
	year = {2014},
	pages = {149--164}
}

@article{henley_k-nearest-neighbour_1996,
	title = {A k-{Nearest}-{Neighbour} {Classifier} for {Assessing} {Consumer} {Credit} {Risk}},
	volume = {45},
	issn = {0039-0526},
	url = {http://www.jstor.org/stable/2348414},
	doi = {10.2307/2348414},
	abstract = {The last 30 years have seen the development of credit scoring techniques for assessing the creditworthiness of consumer loan applicants. Traditional credit scoring methodology has involved the use of techniques such as discriminant analysis, linear or logistic regression, linear programming and decision trees. In this paper we look at the application of the \$k\$-nearest-neighbour (\$k\$-NN) method, a standard technique in pattern recognition and nonparametric statistics, to the credit scoring problem. We propose an adjusted version of the Euclidean distance metric which attempts to incorporate knowledge of class separation contained in the data. Our \$k\$-NN methodology is applied to a real data set and we discuss the selection of optimal values of the parameters \$k\$ and \$D\$ included in the method. To assess the potential of the method we make comparisons with linear and logistic regression and decision trees and graphs. We end by discussing a practical implementation of the proposed \$k\$-NN classifier.},
	number = {1},
	urldate = {2018-04-12},
	journal = {Journal of the Royal Statistical Society. Series D (The Statistician)},
	author = {Henley, W. E. and Hand, D. J.},
	year = {1996},
	pages = {77--95}
}

@inproceedings{sano_stress_2013,
	title = {Stress {Recognition} {Using} {Wearable} {Sensors} and {Mobile} {Phones}},
	doi = {10.1109/ACII.2013.117},
	abstract = {In this study, we aim to find physiological or behavioral markers for stress. We collected 5 days of data for 18 participants: a wrist sensor (accelerometer and skin conductance), mobile phone usage (call, short message service, location and screen on/off) and surveys (stress, mood, sleep, tiredness, general health, alcohol or caffeinated beverage intake and electronics usage). We applied correlation analysis to find statistically significant features associated with stress and used machine learning to classify whether the participants were stressed or not. In comparison to a baseline 87.5\% accuracy using the surveys, our results showed over 75\% accuracy in a binary classification using screen on, mobility, call or activity level information (some showed higher accuracy than the baseline). The correlation analysis showed that the higher-reported stress level was related to activity level, SMS and screen on/off patterns.},
	booktitle = {2013 {Humaine} {Association} {Conference} on {Affective} {Computing} and {Intelligent} {Interaction}},
	author = {Sano, A. and Picard, R. W.},
	month = sep,
	year = {2013},
	keywords = {accelerometer, Accuracy, behavioral marker, binary classification, Biomedical monitoring, classification, correlation analysis, emotion recognition, Feature extraction, higher-reported stress level, learning (artificial intelligence), machine learning, Mobile handsets, mobile phone, mobile phone usage, mobile phones, mobility, Mood, neurophysiology, pattern classification, physiological marker, screen on/off pattern, sensors, Skin, skin conductance, smart phone, smart phones, SMS, stress, Stress, stress recognition, wearable computers, wearable sensor, wearable sensors, wrist sensor},
	pages = {671--676},
	file = {IEEE Xplore Abstract Record:/Users/davidfarrugia/Zotero/storage/E5B4IBWP/6681508.html:text/html}
}

@article{kim_smartphone_2015,
	title = {Smartphone {Addiction}: ({Focused} {Depression}, {Aggression} and {Impulsion}) among {College} {Students}},
	volume = {8},
	issn = {0974 -5645},
	shorttitle = {Smartphone {Addiction}},
	url = {http://www.indjst.org/index.php/indjst/article/view/80215},
	doi = {10.17485/ijst/2015/v8i25/80215},
	abstract = {In this study, a survey was conducted to examine the relationship among smartphone addiction tendency, depression, aggression and impulsion in college students. The data were collected via structural questionnaires completed by 353 college students located in Cheonan who agreed to participate in this study. There was a statistically significant positive correlation between smartphone addiction and depression, and there were positive correlations among smartphone addiction, aggression and impulsion. Hierarchial regression analysis was used to determine the influence of smartphone addiction tendency and to identify its correlation with depression, aggression and impulsions. First step hierarchy that controlled general characteristics shows that gender (p{\textless}.001) influenced smartphone addiction. Explanatory power for explaining the smartphone addiction of the control variables is found to be 5.6\%. Including independent variables Model 2 shows a significant increase and the explanatory power for explaining the smartphone addiction is 31.2\% (p{\textless}.001).},
	language = {en},
	number = {25},
	urldate = {2018-04-12},
	journal = {Indian Journal of Science and Technology},
	author = {Kim, Mi-ok and Kim, Heejeong and Kim, Kyungsook and Ju, Sejin and Choi, Junghyun and Yu, Mi},
	month = sep,
	year = {2015},
	keywords = {Aggression, Depression, Impulsion, Smartphone Game.},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/RZZNXE3W/Kim et al. - 2015 - Smartphone Addiction (Focused Depression, Aggress.pdf:application/pdf}
}

@article{gharehchopogh_new_2015,
	title = {A {New} {Approach} in {Bloggers} {Classification} with {Hybrid} of {K}-{Nearest} {Neighbor} and {Artificial} {Neural} {Network} {Algorithms}},
	volume = {8},
	issn = {0974 -5645},
	url = {http://www.indjst.org/index.php/indjst/article/view/59570},
	doi = {10.17485/ijst/2015/v8i3/59570},
	abstract = {Blogs are one of the effective tools of web2 which are considered as one of the major module and of social and interactive capabilities in making IT world wonderful for the cyber and virtual living. Two methods were used in this paper: K-Nearest Neighbor (KNN) and Artificial Neural Networks (ANNs). These methods are classified based on Kohkiloye and Boyer Ahmad province bloggers dataset considering input features of each blogger to the other methods and previously provided algorithms as more optimal. Our simulation and experiments not only provide hopeful results but also higher anticipation and classification rate.},
	language = {en},
	number = {3},
	urldate = {2018-04-12},
	journal = {Indian Journal of Science and Technology},
	author = {Gharehchopogh, Farhad Soleimanian and Khaze, Seyyed Reza and Maleki, Isa},
	month = feb,
	year = {2015},
	keywords = {Artificial Neural Networks, Bloggers Classification, Decision Tree, K-Nearest Neighbor.},
	pages = {237--246},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/H2DSGYUS/Gharehchopogh et al. - 2015 - A New Approach in Bloggers Classification with Hyb.pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/FKG33G6R/0.html:text/html}
}

@article{fernandes_parametric_2005,
	title = {Parametric (modified least squares) and non-parametric ({Theil}–{Sen}) linear regressions for predicting biophysical parameters in the presence of measurement errors},
	volume = {3},
	issn = {0034-4257},
	url = {https://www.infona.pl//resource/bwmeta1.element.elsevier-3c6592e1-1a6d-3ca4-9eb0-6f35795f1cc3},
	doi = {10.1016/j.rse.2005.01.005},
	abstract = {Remote sensing often involves the estimation of in situ quantities from remote measurements. Linear regression, where there are no non-linear combinations of regressors, is a common approach to this prediction problem in the remote sensing community. A review of recent remote sensing articles using univariate linear regression indicates that in the majority of cases, ordinary least squares (OLS) linear regression has been applied, with approximately half the articles using the in situ observations as regressors and the other half using the inverse regression with remote measurements as regressors. OLS implicitly assume an underlying normal structural data model to arrive at unbiased estimates of the response. OLS regression can be a biased predictor in the presence of measurement errors when the regression problem is based on a functional rather than structural data model. Parametric (Modified Least Squares) and non-parametric (Theil-Sen) consistent predictors are given for linear regression in the presence of measurement errors together with analytical approximations of their prediction confidence intervals. Three case studies involving estimation of leaf area index from nadir reflectance estimates are used to compare these unbiased estimators with OLS linear regression. A comparison to Geometric Mean regression, a standardized version of Reduced Major Axis regression, is also performed. The Theil–Sen approach is suggested as a potential replacement of OLS for linear regression in remote sensing applications. It offers simplicity in computation, analytical estimates of confidence intervals, robustness to outliers, testable assumptions regarding residuals and requires limited a priori information regarding measurement errors.},
	language = {English},
	number = {95},
	urldate = {2018-04-12},
	journal = {Remote Sensing of Environment},
	author = {Fernandes, Richard and Leblanc, Sylvain G.},
	year = {2005},
	pages = {303--316},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/2TBXLZU7/bwmeta1.element.html:text/html}
}

@article{peng_consistency_2008,
	title = {Consistency and asymptotic distribution of the {Theil}–{Sen} estimator},
	volume = {138},
	doi = {10.1016/j.jspi.2007.06.036},
	abstract = {In this paper, we obtain the strong consistency and asymptotic distribution of the Theil–Sen estimator in simple linear regression models with arbitrary error distributions. We show that the Theil–Sen estimator is super-efficient when the error distribution is discontinuous and that its asymptotic distribution may or may not be normal when the error distribution is continuous. We give an example in which the Theil–Sen estimator is not asymptotically normal. A small simulation study is conducted to confirm the super-efficiency and the non-normality of the asymptotic distribution.},
	journal = {Journal of Statistical Planning and Inference},
	author = {Peng, Hanxiang and Wang, Shaoli and Wang, Xueqin},
	month = jul,
	year = {2008},
	pages = {1836--1850}
}

@article{vaidyanathan_comprehensive_2005,
	title = {A {Comprehensive} {Model} for {Software} {Rejuvenation}},
	volume = {2},
	issn = {1545-5971},
	url = {http://dx.doi.org/10.1109/TDSC.2005.15},
	doi = {10.1109/TDSC.2005.15},
	abstract = {Recently, the phenomenon of software aging, one in which the state of the software system degrades with time, has been reported. This phenomenon, which may eventually lead to system performance degradation and/or crash/hang failure, is the result of exhaustion of operating system resources, data corruption, and numerical error accumulation. To counteract software aging, a technique called software rejuvenation has been proposed, which essentially involves occasionally terminating an application or a system, cleaning its internal state and/or its environment, and restarting it. Since rejuvenation incurs an overhead, an important research issue is to determine optimal times to initiate this action. In this paper, we first describe how to include faults attributed to software aging in the framework of Gray's software fault classification (deterministic and transient), and study the treatment and recovery strategies for each of the fault classes. We then construct a semi-Markov reward model based on workload and resource usage data collected from the UNIX operating system. We identify different workload states using statistical cluster analysis, estimate transition probabilities, and sojourn time distributions from the data. Corresponding to each resource, a reward function is then defined for the model based on the rate of resource depletion in each state. The model is then solved to obtain estimated times to exhaustion for each resource. The result from the semi-Markov reward model are then fed into a higher-level availability model that accounts for failure followed by reactive recovery, as well as proactive recovery. This comprehensive model is then used to derive optimal rejuvenation schedules that maximize availability or minimize downtime cost.},
	number = {2},
	urldate = {2018-04-12},
	journal = {IEEE Trans. Dependable Secur. Comput.},
	author = {Vaidyanathan, Kalyanaraman and Trivedi, Kishor S.},
	month = apr,
	year = {2005},
	keywords = {Index Terms- Availability, measurement-based dependability evaluation, semi-Markov reward models, software aging, software rejuvenation, workload characterization.},
	pages = {124--137}
}

@article{romanic_djordje_longterm_2014,
	title = {Long‐term trends of the ‘{Koshava}’ wind during the period 1949–2010},
	volume = {35},
	issn = {0899-8418},
	url = {https://rmets.onlinelibrary.wiley.com/doi/abs/10.1002/joc.3981},
	doi = {10.1002/joc.3981},
	abstract = {ABSTRACT In this study, a comprehensive analysis of long?term trends of the Koshava wind during the period between 1949 and 2010 is carried out. Koshava is a strong wind that blows from southeast quadrant over Serbia, Bulgaria and Romania. The Siberian high and West?Mediterranean cyclones, together with the orography of the eastern Balkan, are the main drivers of the Koshava wind. The trend analyses are performed on wind data sets from five synoptic weather stations, all situated in the region where the Koshava wind is fully developed. Koshava wind speeds are divided into two categories: (1) all wind speeds and (2) wind speeds above 5?ms?1. Two homogeneity tests are used to inspect the quality of wind speed and wind direction time series. The Mann?Kendall test and Sen's slope estimator are used to analyze trends of the Koshava wind speeds and the annual number of days with the Koshava wind. Statistically significant negative trends of the Koshava wind speeds and wind activity are observed at all five weather stations and are more pronounced for wind speeds above 5?ms?1. The negative trends of the Koshava wind are mostly related to the changes in the synoptic circulation, temperature and weakening of the Siberian high and West?Mediterranean cyclones. It is shown that the observed decline of the Koshava wind has a significant impact on reducing the wind energy potential in the region.},
	number = {2},
	urldate = {2018-04-12},
	journal = {International Journal of Climatology},
	author = {{Romanić Djordje} and {Ćurić Mladjen} and {Jovičić Ilija} and {Lompar Miloš}},
	month = mar,
	year = {2014},
	keywords = {Koshava wind, Mann–Kendall test, Serbia, trend analysis, wind direction, wind speed},
	pages = {288--302},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/7KDR42KG/joc.html:text/html}
}

@misc{plot.ly_theil-sen_2015,
	title = {Theil-{Sen} {Regression} in {Scikit}-learn},
	url = {https://plot.ly/scikit-learn/plot-theilsen/},
	urldate = {2018-04-12},
	author = {{plot.ly}},
	year = {2015},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/DUDUMGIB/3266.html:text/html}
}

@misc{noauthor_complete_2016,
	title = {A {Complete} {Tutorial} on {Ridge} and {Lasso} {Regression} in {Python}},
	url = {https://www.analyticsvidhya.com/blog/2016/01/complete-tutorial-ridge-lasso-regression-python/},
	abstract = {Here is a complete tutorial on the regularization techniques of ridge and lasso regression to prevent overfitting in prediction in python},
	urldate = {2018-04-12},
	journal = {Analytics Vidhya},
	month = jan,
	year = {2016},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/28X6LKHK/complete-tutorial-ridge-lasso-regression-python.html:text/html}
}

@misc{chakon_practical_2017,
	title = {Practical machine learning: {Ridge} {Regression} vs. {Lasso}},
	shorttitle = {Practical machine learning},
	url = {https://hackernoon.com/practical-machine-learning-ridge-regression-vs-lasso-a00326371ece},
	abstract = {For many years, programmers have tried to solve extremely complex computer science problems using traditional algorithms which are based on…},
	urldate = {2018-04-12},
	journal = {Hacker Noon},
	author = {Chakon, Ofir},
	month = aug,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/SMW4VFGJ/practical-machine-learning-ridge-regression-vs-lasso-a00326371ece.html:text/html}
}

@article{hadgu_application_1984,
	title = {An application of ridge regression analysis in the study of syphilis data},
	volume = {3},
	issn = {0277-6715},
	abstract = {We provide a brief summary of the theory of ridge regression, regress reported rates of congenital syphilis on three stages of adult syphilis using both ridge regression and least squares techniques, and discuss the results. We also develop a principal components regression model for the same example and compare the results. The example demonstrates the usefulness of ridge regression to handle multicollinearity in data. Problems of overestimation and instability associated with ordinary least squares estimates of these non-orthogonal data diminish with use of the ridge regression techniques.},
	language = {eng},
	number = {3},
	journal = {Stat Med},
	author = {Hadgu, A.},
	month = sep,
	year = {1984},
	pmid = {6484379},
	keywords = {Adult, Humans, Infant, Newborn, Regression Analysis, Syphilis, Syphilis, Congenital, Syphilis, Latent, United States},
	pages = {293--299}
}

@article{coniffe_application_1975,
	title = {Application of {Ridge} regression in agricultural economics},
	issn = {0021-1249},
	url = {http://agris.fao.org/agris-search/search.do?recordID=US201302522072},
	language = {English},
	urldate = {2018-04-12},
	journal = {Irish journal of agricultural economics and rural sociology},
	author = {Coniffe, D. and Stone, J. and O'Neill, F.},
	year = {1975},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/4953A3YV/search.html:text/html}
}

@article{g._easterling_point_1973,
	title = {Point {Estimation} and {Bayesian} {Inference}},
	volume = {27},
	doi = {10.2307/3087416},
	journal = {The American Statistician},
	author = {G. Easterling, R},
	month = dec,
	year = {1973},
	pages = {241}
}

@article{noauthor_error_nodate,
	title = {Error},
	url = {https://onlinelibrary.wiley.com/doi/full/10.1111/abstract},
	language = {en},
	urldate = {2018-04-12}
}

@article{willcox_review_1980,
	title = {A review of numerical methods in bacterial identification},
	volume = {46},
	issn = {0003-6072, 1572-9699},
	url = {https://link.springer.com/article/10.1007/BF00453024},
	doi = {10.1007/BF00453024},
	abstract = {Part A of this review describes the particular computer-assisted identification service operated by the NCTC. In Part B, the use of probability matrices is examined, discussing various methods of calculating likelihoods and the problems that arise when calculating these from probability matrices. Part C describes the alternative numerical methods of constructing identification keys and the supplementary methods of selecting “best sets” of characters to aid identification. Finally, in Part D, the prospects and limitations of numerical methods in bacterial identification are assessed, first with regard to methodology used and then in terms of performance and practical limitations.},
	language = {en},
	number = {3},
	urldate = {2018-04-12},
	journal = {Antonie van Leeuwenhoek},
	author = {Willcox, W. R. and Lapage, S. P. and Holmes, B.},
	month = may,
	year = {1980},
	pages = {233--299},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/8Y4KTJCL/BF00453024.html:text/html}
}

@article{willcox_identification_1973,
	title = {Identification of {Bacteria} by {Computer}: {Theory} and {Programming}},
	volume = {77},
	shorttitle = {Identification of {Bacteria} by {Computer}},
	url = {http://mic.microbiologyresearch.org/content/journal/micro/10.1099/00221287-77-2-317},
	doi = {10.1099/00221287-77-2-317},
	abstract = {: The methods incorporated in the computer program used in a trial of computer-aided identification of bacteria are described. The identification method is based on Bayes's theorem and allows for dependent tests and missing data in the probability matrix. It was found useful in developing the method to take account of the occurrence of errors in bacteriological testing. The method suggests a definite identification only if the Bayesian probability of one of the taxa exceeds a threshold level; if not, a separate procedure selects the best tests to continue the identification.},
	number = {2},
	urldate = {2018-04-12},
	journal = {Microbiology},
	author = {WILLCOX, W. R. and LAPAGE, S. P. and BASCOMB, SHOSHANA and CURTIS, M. A.},
	year = {1973},
	pages = {317--330},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/4D3LTX6B/00221287-77-2-317.html:text/html}
}

@article{adams_computer_1986,
	title = {Computer aided diagnosis of acute abdominal pain: a multicentre study.},
	volume = {293},
	issn = {0267-0623},
	shorttitle = {Computer aided diagnosis of acute abdominal pain},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1341582/},
	abstract = {A multicentre study of computer aided diagnosis for patients with acute abdominal pain was performed in eight centres with over 250 participating doctors and 16,737 patients. Performance in diagnosis and decision making was compared over two periods: a test period (when a small computer system was provided to aid diagnosis) and a baseline period (before the system was installed). The two periods were well matched for type of case and rate of accrual. The system proved reliable and was used in 75.1\% of possible cases. User reaction was broadly favourable. During the test period improvements were noted in diagnosis, decision making, and patient outcome. Initial diagnostic accuracy rose from 45.6\% to 65.3\%. The negative laparotomy rate fell by almost half, as did the perforation rate among patients with appendicitis (from 23.7\% to 11.5\%). The bad management error rate fell from 0.9\% to 0.2\%, and the observed mortality fell by 22.0\%. The savings made were estimated as amounting to 278 laparotomies and 8,516 bed nights during the trial period--equivalent throughout the National Health Service to annual savings in resources worth over 20m pounds and direct cost savings of over 5m pounds. Computer aided diagnosis is a useful system for improving diagnosis and encouraging better clinical practice.},
	number = {6550},
	urldate = {2018-04-12},
	journal = {Br Med J (Clin Res Ed)},
	author = {Adams, I D and Chan, M and Clifford, P C and Cooke, W M and Dallos, V and de Dombal, F T and Edwards, M H and Hancock, D M and Hewett, D J and McIntyre, N},
	month = sep,
	year = {1986},
	pmid = {3094664},
	pmcid = {PMC1341582},
	pages = {800--804},
	file = {PubMed Central Full Text PDF:/Users/davidfarrugia/Zotero/storage/2GQH9CUJ/Adams et al. - 1986 - Computer aided diagnosis of acute abdominal pain .pdf:application/pdf}
}

@article{kirwan_mathematical_1985,
	title = {Mathematical {Models} in {Medical} {Diagnosis}},
	volume = {44},
	issn = {0003-4967},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1001743/},
	number = {10},
	urldate = {2018-04-12},
	journal = {Ann Rheum Dis},
	author = {Kirwan, J R},
	month = oct,
	year = {1985},
	pmid = {null},
	pmcid = {PMC1001743},
	pages = {693},
	file = {PubMed Central Full Text PDF:/Users/davidfarrugia/Zotero/storage/R2NPUZZP/Kirwan - 1985 - Mathematical Models in Medical Diagnosis.pdf:application/pdf}
}

@article{hand_idiots_2001,
	title = {Idiot’s {Bayes} – not so stupid after all?},
	volume = {69},
	journal = {International Statistical Review},
	author = {Hand, David and Yu, Keming},
	month = jan,
	year = {2001}
}

@inproceedings{zhang_optimality_2004,
	title = {The {Optimality} of {Naive} {Bayes}},
	volume = {2},
	abstract = {Naive Bayes is one of the most efficient and effective inductive learning algorithms for machine learning and data mining. Its competitive performance in classifica- tion is surprising, because the conditional independence assumption on which it is based, is rarely true in real- world applications. An open question is: what is the true reason for the surprisingly good performance of naive Bayes in classification? In this paper, we propose a novel explanation on the superb classification performance of naive Bayes. We show that, essentially, the dependence distribution; i.e., how the local dependence of a node distributes in each class, evenly or unevenly, and how the local dependen- cies of all nodes work together, consistently (support- ing a certain classification) or inconsistently (cancel- ing each other out), plays a crucial role. Therefore, no matter how strong the dependences among attributes are, naive Bayes can still be optimal if the dependences distribute evenly in classes, or if the dependences can- cel each other out. We propose and prove a sufficient and necessary conditions for the optimality of naive Bayes. Further, we investigate the optimality of naive Bayes under the Gaussian distribution. We present and prove a sufficient condition for the optimality of naive Bayes, in which the dependence between attributes do exist. This provides evidence that dependence among attributes may cancel out each other. In addition, we explore when naive Bayes works well.},
	booktitle = {Proceedings of the {Seventeenth} {International} {Florida} {Artificial} {Intelligence} {Research} {Society} {Conference}, {FLAIRS} 2004},
	author = {Zhang, Harry},
	month = jan,
	year = {2004}
}

@misc{noauthor_overview_2016,
	title = {An overview of gradient descent optimization algorithms},
	url = {u=http://ruder.io/optimizing-gradient-descent/},
	abstract = {This blog post looks at variants of gradient descent and the algorithms that are commonly used to optimize them.},
	urldate = {2018-04-13},
    author = {Ruder, Sebastian},
	journal = {Sebastian Ruder},
	month = jan,
	year = {2016},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/SESD3ZNE/optimizing-gradient-descent.html:text/html}
}

@misc{vryniotis_tuning_2013,
	title = {Tuning the learning rate in {Gradient} {Descent}},
	url = {http://blog.datumbox.com/tuning-the-learning-rate-in-gradient-descent/},
	journal = {DatumBox},
	author = {Vryniotis, Vasilis},
	month = oct,
	year = {2013}
}

@article{schoenauer-sebag_stochastic_2017,
	title = {Stochastic {Gradient} {Descent}: {Going} {As} {Fast} {As} {Possible} {But} {Not} {Faster}},
	shorttitle = {Stochastic {Gradient} {Descent}},
	url = {http://arxiv.org/abs/1709.01427},
	abstract = {When applied to training deep neural networks, stochastic gradient descent (SGD) often incurs steady progression phases, interrupted by catastrophic episodes in which loss and gradient norm explode. A possible mitigation of such events is to slow down the learning process. This paper presents a novel approach to control the SGD learning rate, that uses two statistical tests. The first one, aimed at fast learning, compares the momentum of the normalized gradient vectors to that of random unit vectors and accordingly gracefully increases or decreases the learning rate. The second one is a change point detection test, aimed at the detection of catastrophic learning episodes; upon its triggering the learning rate is instantly halved. Both abilities of speeding up and slowing down the learning rate allows the proposed approach, called SALeRA, to learn as fast as possible but not faster. Experiments on standard benchmarks show that SALeRA performs well in practice, and compares favorably to the state of the art.},
	urldate = {2018-04-13},
	journal = {arXiv:1709.01427 [cs, stat]},
	author = {Schoenauer-Sebag, Alice and Schoenauer, Marc and Sebag, Michèle},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.01427},
	keywords = {Computer Science - Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {arXiv\:1709.01427 PDF:/Users/davidfarrugia/Zotero/storage/T498KXRF/Schoenauer-Sebag et al. - 2017 - Stochastic Gradient Descent Going As Fast As Poss.pdf:application/pdf;arXiv.org Snapshot:/Users/davidfarrugia/Zotero/storage/JNQXLY4B/1709.html:text/html}
}

@incollection{bottou_stochastic_2012,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Stochastic {Gradient} {Descent} {Tricks}},
	isbn = {978-3-642-35288-1 978-3-642-35289-8},
	url = {https://link.springer.com/chapter/10.1007/978-3-642-35289-8_25},
	abstract = {Chapter 1 strongly advocates the stochastic back-propagation method to train neural networks. This is in fact an instance of a more general technique called stochastic gradient descent (SGD). This chapter provides background material, explains why SGD is a good learning algorithm when the training set is large, and provides useful recommendations.},
	language = {en},
	urldate = {2018-04-13},
	booktitle = {Neural {Networks}: {Tricks} of the {Trade}},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Bottou, Léon},
	year = {2012},
	doi = {10.1007/978-3-642-35289-8_25},
	pages = {421--436},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/SIRWLVFK/978-3-642-35289-8_25.html:text/html}
}

@misc{noauthor_disadvantages_nodate,
	title = {The disadvantages of {Stochastic} {Gradient} {Descent} include {SGD} requires a number},
	url = {https://www.coursehero.com/file/p69g5hu/The-disadvantages-of-Stochastic-Gradient-Descent-include-SGD-requires-a-number/},
	abstract = {The disadvantages of Stochastic Gradient Descent include SGD requires a number from BIOL 320 at Stevens},
	language = {en},
	urldate = {2018-04-13},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/KXATT5QL/The-disadvantages-of-Stochastic-Gradient-Descent-include-SGD-requires-a-number.html:text/html}
}

@article{klein_adaptive_2009,
	title = {Adaptive {Stochastic} {Gradient} {Descent} {Optimisation} for {Image} {Registration}},
	volume = {81},
	issn = {0920-5691, 1573-1405},
	url = {https://link.springer.com/article/10.1007/s11263-008-0168-y},
	doi = {10.1007/s11263-008-0168-y},
	abstract = {We present a stochastic gradient descent optimisation method for image registration with adaptive step size prediction. The method is based on the theoretical work by Plakhov and Cruz (J. Math. Sci. 120(1):964–973, 2004). Our main methodological contribution is the derivation of an image-driven mechanism to select proper values for the most important free parameters of the method. The selection mechanism employs general characteristics of the cost functions that commonly occur in intensity-based image registration. Also, the theoretical convergence conditions of the optimisation method are taken into account. The proposed adaptive stochastic gradient descent (ASGD) method is compared to a standard, non-adaptive Robbins-Monro (RM) algorithm. Both ASGD and RM employ a stochastic subsampling technique to accelerate the optimisation process. Registration experiments were performed on 3D CT and MR data of the head, lungs, and prostate, using various similarity measures and transformation models. The results indicate that ASGD is robust to these variations in the registration framework and is less sensitive to the settings of the user-defined parameters than RM. The main disadvantage of RM is the need for a predetermined step size function. The ASGD method provides a solution for that issue.},
	language = {en},
	number = {3},
	urldate = {2018-04-13},
	journal = {Int J Comput Vis},
	author = {Klein, Stefan and Pluim, Josien P. W. and Staring, Marius and Viergever, Max A.},
	month = mar,
	year = {2009},
	pages = {227},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/9CHSP2DU/Klein et al. - 2009 - Adaptive Stochastic Gradient Descent Optimisation .pdf:application/pdf;Snapshot:/Users/davidfarrugia/Zotero/storage/68QTDHND/s11263-008-0168-y.html:text/html}
}

@article{m._v._s._ratnayake_application_2014,
	title = {Application of {Stochastic} {Gradient} {Descent} {Algorithm} in {Evaluating} the {Performance} {Contribution} of {Employees}},
	volume = {16},
	doi = {10.9790/487X-16637780},
	journal = {IOSR Journal of Business and Management},
	author = {M. V. S. Ratnayake, R and G. U Perera, K and S. K. Wickramanayaka, G and S. Gunasekara, C and C. J. K. Samarawickrama, K},
	month = jan,
	year = {2014},
	pages = {77--80}
}

@inproceedings{zhang_solving_2004,
	address = {New York, NY, USA},
	series = {{ICML} '04},
	title = {Solving {Large} {Scale} {Linear} {Prediction} {Problems} {Using} {Stochastic} {Gradient} {Descent} {Algorithms}},
	isbn = {978-1-58113-838-2},
	url = {http://doi.acm.org/10.1145/1015330.1015332},
	doi = {10.1145/1015330.1015332},
	abstract = {Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings.},
	urldate = {2018-04-13},
	booktitle = {Proceedings of the {Twenty}-first {International} {Conference} on {Machine} {Learning}},
	publisher = {ACM},
	author = {Zhang, Tong},
	year = {2004},
	pages = {116--}
}

@misc{noauthor_learn_2017,
	title = {Learn {Linear} {Regression} using {Excel} - {Machine} {Learning} {Algorithm}},
	url = {https://www.newtechdojo.com/learn-linear-regression-using-excel/},
	abstract = {Beginnerguide to learn the most well known and well-understood algorithm in statistics and machine learning. In this post, you will discover the linear regression algorithm, how it works using Excel, application and pros and cons.},
	language = {en-US},
	urldate = {2018-04-13},
	journal = {New Tech Dojo},
	month = dec,
	year = {2017},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/QV9GYV22/learn-linear-regression-using-excel.html:text/html}
}

@misc{noauthor_disadvantages_nodate-1,
	title = {The {Disadvantages} of {Linear} {Regression}},
	url = {https://sciencing.com/disadvantages-linear-regression-8562780.html},
	abstract = {While linear regression is a useful tool for analysis, it does have its disadvantages, including its sensitivity to outliers and more.},
	language = {en},
	urldate = {2018-04-13},
	journal = {Sciencing},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/6VRQQ7B2/disadvantages-linear-regression-8562780.html:text/html}
}

@misc{noauthor_advantages_nodate,
	title = {The {Advantages} \& {Disadvantages} of a {Multiple} {Regression} {Model}},
	url = {https://sciencing.com/advantages-disadvantages-multiple-regression-model-12070171.html},
	abstract = {When analyzing complex data, it helps to know the advantages and disadvantages of a multiple regression model before making conclusions.},
	language = {en},
	urldate = {2018-04-13},
	journal = {Sciencing},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/TT5MLWYW/advantages-disadvantages-multiple-regression-model-12070171.html:text/html}
}

@article{cook_detection_1977,
	title = {Detection of {Influential} {Observation} in {Linear} {Regression}},
	volume = {19},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.1977.10489493},
	doi = {10.1080/00401706.1977.10489493},
	abstract = {A new measure based on confidence ellipsoids is developed for judging the contribution of each data point to the determination of the least squares estimate of the parameter vector in full rank linear regression models. It is shown that the measure combines information from the studentized residuals and the variances of the residuals and predicted values. Two examples are presented.},
	number = {1},
	urldate = {2018-04-13},
	journal = {Technometrics},
	author = {Cook, R. Dennis},
	month = feb,
	year = {1977},
	keywords = {Confidence ellipsoids, Influential observations, Outliers, Variances of residuals},
	pages = {15--18},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/MDQNQ5AV/00401706.1977.html:text/html}
}

@article{isobe_linear_1990,
	title = {Linear regression in astronomy.},
	volume = {364},
	issn = {0004-637X},
	url = {http://adsabs.harvard.edu/abs/1990ApJ...364..104I},
	doi = {10.1086/169390},
	abstract = {Five methods for obtaining linear regression fits to bivariate data with unknown or insignificant measurement errors are discussed: ordinary least-squares (OLS) regression of Y on X, OLS regression of X on Y, the bisector of the two OLS lines, orthogonal regression, and 'reduced major-axis' regression. These methods have been used by various
researchers in observational astronomy, most importantly in cosmic distance scale applications. Formulas for calculating the slope and intercept coefficients and their uncertainties are given for all the methods, including a new general form of the OLS variance estimates. The accuracy of the formulas was confirmed using numerical simulations. The applicability of the procedures is discussed with respect to their mathematical properties, the nature of the astronomical data under consideration, and the scientific purpose of the regression. It is found that, for problems needing symmetrical treatment of the variables, the OLS bisector performs significantly better than orthogonal or reduced major-axis regression.},
	urldate = {2018-04-13},
	journal = {The Astrophysical Journal},
	author = {Isobe, Takashi and Feigelson, Eric D. and Akritas, Michael G. and Babu, Gutti Jogesh},
	month = nov,
	year = {1990},
	keywords = {Astronomy, Computational Astrophysics, Galaxies, Least Squares Method, Regression Analysis, Slopes},
	pages = {104--113},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/Q9FQ5S5N/Isobe et al. - 1990 - Linear regression in astronomy..pdf:application/pdf}
}

@misc{norris_linear_2008,
	title = {The {Linear} {Regression} of {Time} and {Price}},
	url = {https://www.investopedia.com/articles/trading/09/linear-regression-time-price.asp},
	abstract = {This investment strategy can help investors be successful by identifying price trends while eliminating human bias.},
	language = {en},
	urldate = {2018-04-13},
	journal = {Investopedia},
	author = {Norris, Emily},
	month = nov,
	year = {2008},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/6CWWU8ZM/linear-regression-time-price.html:text/html}
}

@inproceedings{yan_comparison_2011,
	title = {The {Comparison} of {Five} {Discriminant} {Methods}},
	doi = {10.1109/ICMSS.2011.5999201},
	abstract = {The purpose of this article is to study advantages and disadvantages about discriminant analysis with five linear methods. And through comparison,we can obtain that there are not absolute rules to tell us which is best in discriminant analysis with linear methods. The conditions in practice determine mostly the power of five methods. Through this case,we find that FDA is a most stable discriminant analysis,compared with other five methods in this case.},
	booktitle = {2011 {International} {Conference} on {Management} and {Service} {Science}},
	author = {Yan, H. and Dai, Y.},
	month = aug,
	year = {2011},
	keywords = {Barium, Companies, Covariance matrix, discriminant functions, discriminant methods, Educational institutions, FDA, linear discriminant analysis, Linear discriminant analysis, stable discriminant analysis, statistical analysis, Training, Vectors},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/Users/davidfarrugia/Zotero/storage/M6Z45KCE/5999201.html:text/html}
}

@article{li_2d-lda:_2005,
	title = {2D-{LDA}: {A} {Statistical} {Linear} {Discriminant} {Analysis} for {Image} {Matrix}},
	volume = {26},
	issn = {0167-8655},
	shorttitle = {2D-{LDA}},
	url = {http://dx.doi.org/10.1016/j.patrec.2004.09.007},
	doi = {10.1016/j.patrec.2004.09.007},
	abstract = {This paper proposes an innovative algorithm named 2D-LDA, which directly extracts the proper features from image matrices based on Fisher's Linear Discriminant Analysis. We experimentally compare 2D-LDA to other feature extraction methods, such as 2D-PCA, Eigenfaces and Fisherfaces. And 2D-LDA achieves the best performance.},
	number = {5},
	urldate = {2018-04-13},
	journal = {Pattern Recogn. Lett.},
	author = {Li, Ming and Yuan, Baozong},
	month = apr,
	year = {2005},
	keywords = {Face recognition, Feature extraction, Image representation, Linear discriminant analysis, Subspace techniques},
	pages = {527--532}
}

@article{fisher_use_1936,
	title = {The {Use} of {Multiple} {Measurements} in {Taxonomic} {Problems}.},
	shorttitle = {138},
	url = {https://digital.library.adelaide.edu.au/dspace/handle/2440/15227},
	abstract = {Reproduced with permission of Cambridge University Press},
	language = {en},
	urldate = {2018-04-13},
	author = {Fisher, Ronald Aylmer},
	year = {1936},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/TW78Y7RL/Fisher - 1936 - 138 The Use of Multiple Measurements in Taxonomic.pdf:application/pdf}
}

@article{phinyomark_application_2012,
	title = {Application of {Linear} {Discriminant} {Analysis} in {Dimensionality} {Reduction} for {Hand} {Motion} {Classification}},
	volume = {12},
	issn = {1335-8871},
	url = {https://www.degruyter.com/view/j/msr.2012.12.issue-3/v10048-012-0015-8/v10048-012-0015-8.xml},
	doi = {10.2478/v10048-012-0015-8},
	abstract = {The classification of upper-limb movements based on surface electromyography (EMG) signals is an important issue in the control of assistive devices and rehabilitation systems. Increasing the number of EMG channels and features in order to increase the number of control commands can yield a high dimensional feature vector. To cope with the accuracy and computation problems associated with high dimensionality, it is commonplace to apply a processing step that transforms the data to a space of significantly lower dimensions with only a limited loss of useful information. Linear discriminant analysis (LDA) has been successfully applied as an EMG feature projection method. Recently, a number of extended LDA-based algorithms have been proposed, which are more competitive in terms of both classification accuracy and computational costs/times with classical LDA. This paper presents the findings of a comparative study of classical LDA and five extended LDA methods. From a quantitative comparison based on seven multi-feature sets, three extended LDA-based algorithms, consisting of uncorrelated LDA, orthogonal LDA and orthogonal fuzzy neighborhood discriminant analysis, produce better class separability when compared with a baseline system (without feature projection), principle component analysis (PCA), and classical LDA. Based on a 7-dimension time domain and time-scale feature vectors, these methods achieved respectively 95.2\% and 93.2\% classification accuracy by using a linear discriminant classifier.},
	number = {3},
	urldate = {2018-04-13},
	journal = {Measurement Science Review},
	author = {Phinyomark, A. and Hu, H. and Phukpattaranont, P. and Limsakul, C.},
	year = {2012},
	keywords = {Electromyography signal, EMG, feature extraction, feature projection, kernel discriminant analysis, orthogonal fuzzy neighborhood discriminant analysis, orthogonal LDA, QR decomposition, uncorrelated LDA},
	pages = {82--89},
	file = {Full Text PDF:/Users/davidfarrugia/Zotero/storage/HPSYE8H7/Phinyomark et al. - 2012 - Application of Linear Discriminant Analysis in Dim.pdf:application/pdf}
}

@inproceedings{alexandre-cortizo_application_2005,
	title = {Application of {Fisher} {Linear} {Discriminant} {Analysis} to {Speech}/{Music} {Classification}},
	volume = {2},
	doi = {10.1109/EURCON.2005.1630291},
	abstract = {This paper proposes the application of Fisher linear discriminants to the problem of speech/music classification. Fisher linear discriminants can classify between two different classes, and are based on the calculation of some kind of centroid for the training data corresponding with each one of these classes. Based on that information a linear boundary is established, which will be used for the classification process. Some results will be given demonstrating the superior behavior of this classification algorithm compared with the well-known K-nearest neighbor algorithm. It will also be demonstrated that it is possible to obtain very good results in terms of probability of error using only one feature extracted from the audio signal, being thus possible to reduce the complexity of this kind of systems in order to implement them in real-time},
	booktitle = {{EUROCON} 2005 - {The} {International} {Conference} on "{Computer} as a {Tool}"},
	author = {Alexandre-Cortizo, E. and Rosa-Zurera, M. and Lopez-Ferreras, F.},
	month = nov,
	year = {2005},
	keywords = {automatic audio classification, Classification algorithms, Discrete Fourier transforms, Face recognition, feature extraction, Feature extraction, Fisher linear discriminant analysis, Linear discriminant analysis, Multiple signal classification, Music, music classification, Real time systems, signal classification, Speech analysis, speech classification, speech processing, Speech/music discrimination, statistical analysis, Training data},
	pages = {1666--1669},
	file = {IEEE Xplore Abstract Record:/Users/davidfarrugia/Zotero/storage/RB2TXGAB/1630291.html:text/html}
}

@article{wilcox_simulations_1998,
	title = {Simulations on the {Theil}-{Sen} regression estimator with right-censored data},
	volume = {39},
	doi = {10.1016/S0167-7152(98)00022-4},
	abstract = {This note compares the small-sample efficiency of the extended Theil-Sen estimator to the modified Buckley-James estimator when the predictor is random. Included are situations where the error term is heteroscedastic. In terms of their standard errors, the extended Theil-Sen estimator is found to offer a substantial advantage in various situations, while the modified Buckley-James estimator never offers an advantage when there is 20\% or 50\% censoring.},
	journal = {Statistics \& Probability Letters},
	author = {Wilcox, Rand},
	month = jul,
	year = {1998},
	pages = {43--47}
}

@article{braverman_using_2013,
	title = {Using cross-game behavioral markers for early identification of high-risk internet gamblers},
	volume = {27},
	issn = {1939-1501},
	doi = {10.1037/a0032818},
	abstract = {Using actual gambling behavior provides the opportunity to develop behavioral markers that operators can use to predict the development of gambling-related problems among their subscribers. Participants were 4,056 Internet gamblers who subscribed to the Internet betting service provider bwin.party. Half of this sample included multiple platform gamblers who were identified by bwin.party's Responsible Gambling (RG) program; the other half were controls randomly selected from those who had the same first deposit date. Using the daily aggregated Internet betting transactions for gamblers' first 31 calendar days of online betting activities at bwin.party, we employed a 2-step analytic strategy: (a) applying an exploratory chi-squared automatic interaction detection (CHAID) decision tree method to identify characteristics that distinguished a subgroup of high-risk Internet gamblers from the rest of the sample, and (b) conducting a confirmatory analysis of those characteristics among an independent validation sample. This analysis identified two high-risk groups (i.e., groups in which 90\% of the members were identified by bwin.party's RG program): Group 1 engaged in three or more gambling activities and evidenced high wager variability on casino-type games; Group 2 engaged in two different gambling activities and evidenced high variability for live action wagers. This analysis advances an ongoing research program to identify potentially problematic Internet gamblers during the earliest stages of their Internet gambling. Gambling providers and public policymakers can use these results to inform early intervention programs that target high-risk Internet gamblers.},
	language = {eng},
	number = {3},
	journal = {Psychol Addict Behav},
	author = {Braverman, Julia and LaPlante, Debi A. and Nelson, Sarah E. and Shaffer, Howard J.},
	month = sep,
	year = {2013},
	pmid = {24059836},
	keywords = {Adult, Behavior, Addictive, Case-Control Studies, Decision Trees, Early Diagnosis, Female, Gambling, Humans, Internet, Male, Mass Screening, Risk, Young Adult},
	pages = {868--877}
}

@misc{cambridge_health_alliance_division_nodate,
	title = {Division on {Addiction}},
	url = {http://www.divisiononaddiction.org/},
	language = {en-US},
	urldate = {2018-04-16},
	author = {{Cambridge Health Alliance}},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/NN5CGJRV/www.divisiononaddiction.org.html:text/html}
}

@misc{bwin.party_gvc_nodate,
	title = {{GVC} {Holdings} {PLC} :: {Corporate} {Website} {\textbar} {We} are a leading provider of {B}2B and {B}2C services to the online gaming and sports betting markets},
	shorttitle = {{GVC} {Holdings} {PLC}},
	url = {https://gvc-plc.com/},
	language = {en-US},
	urldate = {2018-04-16},
	author = {{bwin.party}},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/UE2MQVJB/gvc-plc.com.html:text/html}
}

@misc{vanschoren_openml_nodate,
	title = {{OpenML}},
	url = {https://www.openml.org},
	abstract = {OpenML: exploring machine learning better, together. An open science platform for machine learning.},
	language = {en},
	urldate = {2018-04-18},
	journal = {OpenML: exploring machine learning better, together.},
	author = {Vanschoren, Joaquin}
}

@book{kaufman_finding_1990,
	title = {Finding {Groups} in {Data}: {An} {Introduction} {To} {Cluster} {Analysis}},
	isbn = {978-0-471-87876-6},
	shorttitle = {Finding {Groups} in {Data}},
	abstract = {This is a book. We are not allowed to upload or share it, sorry.},
	author = {Kaufman, Leonard and Rousseeuw, Peter},
	month = jan,
	year = {1990},
	doi = {10.2307/2532178}
}

@misc{noauthor_what_nodate,
	title = {What is a good classification accuracy in data mining? {\textbar} {Data} {Mining} {Blog} - www.dataminingblog.com},
	shorttitle = {What is a good classification accuracy in data mining?},
	url = {http://www.dataminingblog.com/what-is-a-good-classification-accuracy-in-data-mining/},
	abstract = {What a good question! Or what a bad question should I say. In fact, this question is not a good one since if we ask it this way, we might expect an answer that},
	language = {en-US},
	urldate = {2018-04-20},
	file = {Snapshot:/Users/davidfarrugia/Zotero/storage/2CAEEBY2/what-is-a-good-classification-accuracy-in-data-mining.html:text/html}
}

@misc{zygmunt_z._extreme_2014,
	title = {Extreme {Learning} {Machines}},
	url = {http://fastml.com/extreme-learning-machines/},
	language = {English},
	journal = {FastML},
	author = {{Zygmunt Z.}},
	month = jun,
	year = {2014}
}